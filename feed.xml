<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Simcookies</title>
    <description>All about Simcookies Project.</description>
    <link>https://simcookies.github.io/</link>
    <atom:link href="https://simcookies.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 23 Jul 2021 15:42:33 +0900</pubDate>
    <lastBuildDate>Fri, 23 Jul 2021 15:42:33 +0900</lastBuildDate>
    <generator>Jekyll v4.1.1</generator>
    
      <item>
        <title>关于「机器学习和统计学的区别」的思考</title>
        <description>&lt;p&gt;在自学/整理机器学习的时候, 我逐步地发现: 对于同样一个问题, 经常出现既有统计学的解释, 也有机器学习的解释的情况. 对于我个人来说, 很多时候会把两种解释, 两套名词混在一起, 导致逻辑不清晰, 甚至出错的情况. 并且我发现好像有部分人也和我有同样的困惑. 在查阅各种资料之后, 终于这篇文章 &lt;a href=&quot;https://towardsdatascience.com/the-actual-difference-between-statistics-and-machine-learning-64b49f07ea3&quot;&gt;The Actual Difference between Statics and Machine Learning&lt;/a&gt; 解决了我很多困惑. 因此我总结了一些自己看完文章之后的一些复盘还有思考.&lt;/p&gt;

&lt;h1 id=&quot;文章内容回味&quot;&gt;文章内容回味&lt;/h1&gt;

&lt;p&gt;首先大概总结一下文章中的核心观点. 文章拿线性回归举了一个简单的列子. 首先, 线性回归本身是一种分析自变量与因变量之间关系的统计方法. 通过这个方法, 我们既可以&lt;strong&gt;训练一个线性回归器&lt;/strong&gt;(Linear Regressor), 也可以通过最小二乘法&lt;strong&gt;拟合一个统计回归模型&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;训练线性回归器&lt;/strong&gt;做的事情是 “训练模型”, 这个过程只用到了数据的一部分, 也就是训练数据集(Train Dataset). 而训练得到的模型的性能则需要通过数据的另一部分, 也就是测试数据集(Test Dataset), 进行模型的精度评价. 所以机器学习的最终的目的是获得一个“在测试集上能够表现最佳”的模型. 在那之后可能会使用这个模型去预测未来新的数据.&lt;/p&gt;

&lt;p&gt;而&lt;strong&gt;拟合统计模型&lt;/strong&gt;的目的是“建立模型”, 这个过程是不需要训练集和测试集的. 而建模的目的是为了描述特征量数据和目标变量之间的关系, 并不是为了预测未来的数据. 这个过程我们称之为&lt;strong&gt;统计推断(Statistical Inference)&lt;/strong&gt;. &lt;strong&gt;虽然建立的模型可以用来预测, 但是预测不是目的&lt;/strong&gt;. 同时, 评价模型的方法也不再是使用测试集, 而是采用统计学中常出现的参数的显著性和健壮性进行评价.&lt;/p&gt;

&lt;p&gt;上述两者放在一起说: 机器学习的目的是获得一个可以预测未来的模型, 而很少关心该模型的可解释性.换句话说, 就是成果主义. 我们只关心你这个模型能不能预测地准确未知数据. 再换句话说, 只要预测地准确, 我们甚至毫不关心模型能否写成容易理解地数学表达式 (涉及到神经网络的时候, 也确实没有办法写成表达式, 是一个完全的黑盒子). 而统计推断更在乎的是寻找出藏在数据之后的变量之间的关系. 只是掌握了这个内在关系之后, 恰好可以用其来预测而已.&lt;/p&gt;

&lt;h1 id=&quot;个人体验&quot;&gt;个人体验&lt;/h1&gt;

&lt;p&gt;总结上述的内容就是:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;统计模型是为了推理而设计, 而机器学习是为了获得准确的预测.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;确实在学习初期可能并不明白这句话到底在说什么, 但是越在工作中使用机器学习和统计学越能感觉到这句话很对.&lt;/p&gt;

&lt;p&gt;我本人的工作会涉及到生产环节的质量保证. 为了找到影响一个产品正品率的影响因素, 我需要收集生产过程中大量的数据, 比如环境温度, 湿度, 治具表面温度, 化学反应时间等等, 大大小小的生产过程总共有上百个特征量. 一个最理想的状态, 我当然希望能够找到一个可解释的模型, 比如:&lt;/p&gt;

\[Q=k_1\times温度+k_2\times湿度+k_3\times表面温度+k_4\times反应时间+\dots\notag\]

&lt;p&gt;也就是能够找到这里的系数 $(k_1, k_2, k_3,\dots)$.  通多各个特征量去预测一个产品是为正品的概率 $Q$. 根据上述的总结内容, 更适合采用拟合统计模型的方式去解决这样的问题. 得到每一个生产环节特征量对最终品质的影响程度, 就可以向工厂人员反映, 告诉他们哪些环节需要严格控制, 哪些环节影响不大. 总体的效果就是提高了工厂人员工作的效率以及提高了产品正品率. 可是实际上由于采集的数据过于庞大, 实际模型过于复杂, 很难用这么一个清晰而简单的统计模型去描述内在关系. 我们只能得到一个这样一个黑盒模型:&lt;/p&gt;

\[Q=f(温度,适度,表面温度,反应时间,\dots\notag)\]

&lt;p&gt;之后我使用这个黑盒模型去预测一个产品是正品还是次品, 次品的话处理掉, 以保证最终出厂的都是正品. 能实现成这样效果就已经能够带了巨大的成本削减, 足够让人开心了, 也就没有太强烈的需求去搞清楚各个特征量和品质之间的关系到底如何了(还要什么自行车:joy:)&lt;/p&gt;

&lt;p&gt;由此也引出了一个很重要的思考, 在确定启动一个课题之前, 一定要捋清楚「本课题最终的目的是什么」(统计推断还是结果预测), 以及「达到什么样的水平才能算完成」(模型参数显著性水平/置信区间或者是测试集预测正确率)这两个大问题. 不同的目的使得采用的手段不同, 不同的手段需要的知识体系很可能就不一样. 只有脑袋里对最终的目的很清晰, 才能让自己课题推进的过程中不至于混乱.&lt;/p&gt;

&lt;h1 id=&quot;类似的困惑&quot;&gt;类似的困惑&lt;/h1&gt;

&lt;p&gt;在数据科学学习的初期, 相信大部分人和我一样, 对于AI/机器学习/深度学习/数据科学/统计学等名词的关系也产生过困惑. 这些名词的比较类似于上述的比较, 很多时候明白了其中的目的自然就能分清楚了.&lt;/p&gt;

&lt;p&gt;首先 AI 或者叫人工智能, 目的是为了实现一个拥有人类智能的机器. 它是一个超大型的交叉学科, 这是毋庸置疑的. 按照 Wikipedia 给出的定义, AI 这个研究领域包括了一些子课题或者说分支: 演绎/推理/解决问题, 知识表示, 规划, 学习, 运动和控制, 知觉, 社交, 创造力, 伦理等等很多方面, 其中学习指的就是机器学习. 因此说机器学习是AI中的一部分或者说一个研究分支.&lt;/p&gt;

&lt;p&gt;机器学习的主要目的是为了让机器从输入的数据中获得知识(也就是训练模型), 从而让机器自动地去判断和输出相应的结果(也就是预测). 机器学习中也会有各种各样不同的问题, 按照学习过程中是”否有反馈”这一属性, 可以分为监督学习(有反馈), 非监督学习(无反馈)以及强化学习(执行很多步骤之后有反馈). 为了解决大量的机器学习的问题, 前人的研究已经给出了很多效果很好的算法和模型, 包括了前面文章中介绍的&lt;a href=&quot;/2020/11/02/summary-of-machine-learning-algorithms-decision-tree&quot;&gt;决策树模型&lt;/a&gt;还有&lt;a href=&quot;/2021/07/17/summary-of-machine-learning-algorithms-linear-model-1&quot;&gt;线性回归模型&lt;/a&gt;. 其中有一种被广泛使用的就是神经网络模型, 它是仿造人体的神经网络的算法, 也如人体神经一样是有”层”的概念的. 神经网络的层数越多, 称模型越深. 一般在图像处理, 自然语言处理领域神经网络都非常地深. 涉及到包含深层的神经网络的问题都可以使用深度学习来概括. 综上, 深度学习是机器学习的神经网络算法的一个特殊体现, 是其中的一个部分.&lt;/p&gt;

&lt;p&gt;最后是数据科学. 其实如同这个名称的字面意思, 数据科学是紧扣数据的. 其目的最主要的是通过分析已有的数据, 得出一些事物背后的因果/相关关系, 或者做出一些对未来的预测, 以便给出下一步的决策. 为了实现这样的目的, 需要对数据进行预处理, 可视化, 寻找并学习出最适合模型或者对数据进行相关分析和因果推断. 最终将得出的结论反映或者共享于他人. 在这个过程中, 机器学习/统计学只是一种手段, 而不是目的. 并且有一点需要注意的是, 数据科学最重要的是&lt;strong&gt;需要一定的专业领域知识(Domain Knowledge)&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;最终祭出我心中的, 描述 AI/机器学习/深度学习/数据科学/统计学之间关系的一张图(仅代表个人意见)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20210723145550.png&quot; alt=&quot;关系图&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;追溯缘由&quot;&gt;追溯缘由&lt;/h1&gt;

&lt;p&gt;回到标题. 归根到底, 为什么会产生这样的混乱?  最开始产生这样的困惑是还是在线性回归里面. 传统的统计学基于均值方差, 利用线性代数理论一下子就计算得出了回归模型的各个参数. 并且这个计算过程没有其他的选择, 只有唯一的解, 不存在过拟合或者欠拟合的概念. 但是机器学习不一样, 我们使用均值方差作为损失函数, 需要调整模型的超参数, 使得模型在数据集上表现出最小的损失. 找到的那一组超参数碰巧(理论上当然不是碰巧)得到了和传统统计学一样的模型结果. 在这个过程中, 我们使用了梯度下降法, 超参数是不断变化的, 会有很多个解, 根据对数据的拟合的程度来分就自然有了过拟合和欠拟合.&lt;/p&gt;

&lt;p&gt;正是因为两种方法最终得到一种模型, 自然就会让我们初学者怀疑: 我刚才刚了什么? 统计推断出来的还是机器学习出来的? 尤其是现在 Scikit-Learn 这样现有的机器学习包, 帮我们做完所有的事. 会让我们更加模糊. 所以这也成了我非要将算法一个一个拆开看并整理出来的原因. 同时涉及到图像处理, 自然语言处理问题的时候, 就不是统计学建模能够解决的了, 这个时候机器学习和统计学的不同就能够深刻感受到了. (简单点说, 就是现在的模型太简单的原因 :joy: .)&lt;/p&gt;

&lt;h1 id=&quot;结论&quot;&gt;结论&lt;/h1&gt;

&lt;p&gt;正如文章中所说, 两种方法本质都是基于统计学的. 在数据科学家的眼里, 更多偏向于模型拥有合理的解释, 所以统计学似乎成了更好的选择. 但是正如我在实际工作中遇到的问题那样, 在这个大数据时代, 统计学已经不能满足庞大数据的处理了, 这个时候机器学习就自然更适合了. 不过, 无论选择哪一个, 理清楚自己的目的, 再选择手段永远是最重要的.&lt;/p&gt;
</description>
        <pubDate>Fri, 23 Jul 2021 15:40:18 +0900</pubDate>
        <link>https://simcookies.github.io/2021/07/23/machine-learning-vs-statistics</link>
        <guid isPermaLink="true">https://simcookies.github.io/2021/07/23/machine-learning-vs-statistics</guid>
        
        
        <category>machine learning</category>
        
      </item>
    
      <item>
        <title>机器学习算法系列 - 最小二乘法/岭回归/Lasso</title>
        <description>&lt;p&gt;&lt;strong&gt;普通最小二乘法&lt;/strong&gt;(Ordinary Least Squares, OLS), &lt;strong&gt;岭回归&lt;/strong&gt;(Ridge Regression), 最小绝对值收留和选择算子(Least absolute shrinkage and selection operator, &lt;strong&gt;Lasso&lt;/strong&gt;), 这三个算法是线性模型中比较基本的算法. 这一篇文章里一次性对这三个进行一个简单的总结.&lt;/p&gt;

&lt;p&gt;首先, 因为三者都是线性模型, 所以首先假设我们该模型为:&lt;/p&gt;

\[y(w,x)=w_0+w_1x_1+\dots+w_mx_m\notag\]

&lt;p&gt;其中 $w=(w_1,\dots,w_m)$ 称为系数 (coefficient), 而 $w_0$ 称为截距 (intercept), 而三个算法就是为了找出系数和截距. 使用矩阵的方式表达显得更亲切:&lt;/p&gt;

\[y=\begin{bmatrix}y_1\\y_2\\\vdots\\y_n\end{bmatrix},
X=\begin{bmatrix}
1&amp;amp;x_{11}&amp;amp;x_{12}&amp;amp;\dots&amp;amp;x_{1m}\\
1&amp;amp;x_{21}&amp;amp;x_{22}&amp;amp;\dots&amp;amp;x_{2m}\\
\vdots&amp;amp;\vdots&amp;amp;\vdots&amp;amp;\dots&amp;amp;\vdots\\
1&amp;amp;x_{n1}&amp;amp;x_{n2}&amp;amp;\dots&amp;amp;x_{nm}
\end{bmatrix},w=\begin{bmatrix}w_0\\w_1\\w_2\\\vdots\\w_m\end{bmatrix}\\\notag
y=Xw\]

&lt;h1 id=&quot;普通最小二乘法&quot;&gt;普通最小二乘法&lt;/h1&gt;

&lt;p&gt;最小二乘法可能是我接触到最早的机器学习的算法了. 研究生期间经常通过推导出的传递函数去拟合电路仿真的理想结果, 从而确定电路参数, 这本身就是最小二乘法的一个非常重要的应用 – &lt;a href=&quot;https://zh.wikipedia.org/wiki/%E6%9B%B2%E7%B7%9A%E6%93%AC%E5%90%88&quot;&gt;曲线拟合&lt;/a&gt;. 而最小二乘法中的线性或者说普通的最小二乘法, 作为统计回归分析中的一个手段, 是比较容易理解的一个算法. 其可以分解成:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;随机给出一组系数 $w$;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;使用 $w$ 结合数据集的特征量 $X$ 计算出预测值 $Xw$;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;计算预测值和实际值之间的差, 即模型的损失函数, 这里特指残差平方和 (也就是L2范数) $e^2=\left|y-Xw\right|_2^2$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;求出使得残差平方和取得最小值的 $w$&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, 也可以使用更简单的矩阵的方法求出 $w$.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;用一个简单的图表示出来就是:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20210619195655.png&quot; alt=&quot;OLS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;一堆蓝色的点为观测数据, 看着是满足线性关系的感觉. 而橙色的直线就近似的表达了 $x$ 和 $y$ 之间这种线性的关系, 蓝色短线就是预测值和实际数据之间残差大小. 使得所有蓝线距离之和最小的参数就是我们要找的参数. 具体的数学表达为:&lt;/p&gt;

\[\min_{w}\left\|y-Xw\right\|_2^2=\min_w\sum_{i=1}^n\left(y_i-x_i^Tw\right)^2\notag\\
\rightarrow \hat w=(X^TX)^{-1}X^Ty\]

&lt;p&gt;这里的 $\hat w$ 是利用矩阵的方式求得的参数结果.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;矩阵的解法(TL;DR)&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;假设满足使得 $X$ 和 $y$ 线性关系的参数矩阵为 $\hat w$. 那么就有:&lt;/p&gt;

\[\begin{align*}
X\hat w &amp;amp;= y\\
X^TX\hat w &amp;amp;= X^Ty\\
\hat w&amp;amp;=(X^TX)^{-1}X^Ty
\end{align*}\]

&lt;p&gt;而最后一步成立的条件就是 $X^TX$ 是可逆的, 也就是说是满秩的. 但是如果特征量之间的相关性很强时 (也称之为变量的共线性), $X^TX$ 就会很小, 甚至趋于0. 这个时候就需要对这个矩阵做些处理了, 也就带来了岭回归.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;在 Scikit-Learn 中, 实现普通最小二乘法算法的是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LinearRegression&lt;/code&gt; 模型.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearRegression&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_regression&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 生成数据
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_regression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_informative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;noise&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 训练模型
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 配置参数并绘图
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coef_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intercept_&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_dash&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_dash&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;C1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;模型的参数反映在 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;coef_&lt;/code&gt; 和 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;intercept_&lt;/code&gt; 中, 有了这两个值就得到了近似直线. 绘图结果:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20210619203040.png&quot; alt=&quot;OLS_sklearn&quot; /&gt;&lt;/p&gt;

&lt;p&gt;最小二乘法估算系数的手段在统计学中也称为&lt;strong&gt;最小二乘估计&lt;/strong&gt;. 正如上面提到的为了能够使用最小二乘估计得到唯一的参数解, 特征量 $X$ 必须满足线性无关. 如果线性相关了, 得到的解会对特征量非常的敏感, 这种情况称为&lt;strong&gt;多重共线性&lt;/strong&gt; (multicollinearity), 因此数据一定要经过处理, &lt;strong&gt;特别是要去除掉高度相关的特征量&lt;/strong&gt;, 也就出现了改良的算法.&lt;/p&gt;

&lt;h1 id=&quot;岭回归&quot;&gt;岭回归&lt;/h1&gt;

&lt;p&gt;有时也被称作脊回归、吉洪诺夫正则化(Tikhonov regularization). 可以看作是最小二乘法的改良版. 其目的在于改善普通最小二乘法的多重共线性问题. 主要做法是向损失函数增加一个L2范数的惩罚项:&lt;/p&gt;

\[\begin{align*}
&amp;amp;\min_w\left\|Xw-y\right\|_2^2+\lambda\left\|w\right\|_2^2\\
=&amp;amp;\min_w\sum_{i=1}^m\left(y_i-x_i^Tw\right)^2+\lambda\sum_{i=1}^nw_i^2
\end{align*}\]

&lt;p&gt;其中 $\lambda\geq0$ (有时也用 $\alpha$, 两者是等价的) 称为正则化参数, 复杂系数或者岭参数, 该值越大对于异常的 $w$ 的惩罚越大, 使得训练得到的系数对于多重共线性问题更加的稳健. 以矩阵的形式表达出来:&lt;/p&gt;

\[\hat w =(X^TX+\lambda I)^{-1}X^Ty\notag\]

&lt;p&gt;L2范数惩罚项的加入, 使得原本的 $(X^TX)$ 变成了 $(X^TX+\lambda I)$ 从而保证了满秩和可逆. 但同时也放弃了最小二乘法的无偏性, 以降低精度为代价解决了病态矩阵问题. (好像因为 $I$ 是单位矩阵, 只有对角线有值, 所以名字叫岭回归.)&lt;/p&gt;

&lt;p&gt;另外为了从物理角度理解岭回归, 上述的损失函数还可以等价为:&lt;/p&gt;

\[f(w)=\sum_{i=1}^m\left(y_i-x_i^Tw\right)^2\\\notag
s.t.\sum_{i=1}^nw_i^2\leq t\]

&lt;p&gt;可以理解为在一个约束条件下的最小二乘法. 假设变量的个数为两个, 那么残差平方和就可以表示为 $w_1, w_2$ 的一个二次函数, 它是一个在三维空间中的抛物面, 可以使用等值线表示. 如果没有限制条件, 那么通过梯度下降法一定能够找到位于椭圆中心的那个最优值点. 而限制条件 $w_1^2+w_2^2\leq t$ 则相当于二维平面中的一个圆, 限制了两个变量的取值范围. 所以当等值线与圆相切的时候, 才能得到了约束条件下的最优点.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20210620101826.png&quot; alt=&quot;ridge_geometry&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;在 Scikit-learn 中, 实现岭回归的方式是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ridge&lt;/code&gt; 模型. 使用方法和 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LinearRegression&lt;/code&gt; 基本一致, 只是需要一个岭参数 $\alpha$, 这里不再赘述. 需要关注的官方文档中给出的一个参数值随正则化参数 $\alpha$ 变化而变化的趋势图(&lt;a href=&quot;https://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html#sphx-glr-auto-examples-linear-model-plot-ridge-path-py&quot;&gt;参考连接&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20210717203842.png&quot; alt=&quot;sphx_glr_plot_ridge_path_001&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这个图可以看出, 对于一个病态矩阵的问题 (任何一个元素的些许变动, 比如说数据的噪声, 导致整个矩阵的行列式值和逆矩阵发生很大的变化), 当正则化参数取值很大时(图的左侧), 损失函数受到惩罚项的影响就会占到主要作用, 各个特征参数都表现会&lt;u&gt;接近于 0&lt;/u&gt;, 呈现出欠拟合状态; 而取值很小时(接近于0, 图的右侧), 损失函数则趋于最小二乘法的损失函数, 呈现出过拟合状态, 并且此时特征参数表现出了很大的震荡.&lt;/p&gt;

&lt;p&gt;这再次说明了最小二乘法在面临病态矩阵问题时, 受到数据噪声的影响太大. 而岭回归则通过引入L2范数的惩罚项使得影响减弱. &lt;strong&gt;正是这个特性, 使得岭回归相比于最小二乘法更适用于共线性和病态数据的拟合, 常用于多维问题.&lt;/strong&gt; 不过同时需要注意的是, 作为超参数的 $\alpha$, 取值大小的平衡是非常重要的. 而 Scikit-learn 中内置的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RidgeCV&lt;/code&gt; 是集成了交叉验证的岭回归模型训练器, 能够帮助我们找到适合的正则化参数.&lt;/p&gt;

&lt;h1 id=&quot;lasso&quot;&gt;Lasso&lt;/h1&gt;

&lt;p&gt;也叫套索回归, 也是OLS的改良版. 和岭回归的区别是, Lasso向损失函数增加一个L1范数的惩罚项.&lt;/p&gt;

\[\begin{align*}
&amp;amp;\min_w\left\|Xw-y\right\|_2^2+\lambda\left\|w\right\|_1^2\\
=&amp;amp;\min_w\sum_{i=1}^m\left(y_i-x_i^Tw\right)^2+\lambda\sum_{i=1}^n|w_i|
\end{align*}\]

&lt;p&gt;因为L1范数是绝对值的形式, 损失函数在零点处不可导, 所以不能求得解析解. 所以使用梯度下降法求解最小值. 和岭回归一样, 从物理角度去看, 上述的式子等价为:&lt;/p&gt;

\[f(w)=\sum_{i=1}^m\left(y_i-x_i^Tw\right)^2\\\notag
s.t.\sum_{i=1}^n|w_i|\leq t\]

&lt;p&gt;也是一个在约束条件下求解最小值的问题. 假设变量也是两个, 则限制条件 $|w_1|+|w_2|\leq t$ 则相当于二维平面中的一个矩形. 两者相交时得到最优点.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20210711235439.png&quot; alt=&quot;Lasso_geometry&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Scikit-learn中使用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Lasso&lt;/code&gt; 模型实现 Lasso 算法. 使用方法也是传入一个正则化参数即可, 这里不赘述.&lt;/p&gt;

&lt;p&gt;需要注意的是, 与岭回归不同的地方在于, Lasso回归引入的L1范数惩罚项会使得某些特征参数的值&lt;u&gt;变成 0&lt;/u&gt;(岭回归只会使得参数更容易趋于0但不等于0). 那么为什么L1范数会比L2范数更能使得参数为 0 呢? 因为从上面的两张物理意义图中可以看出, 概率上方形的约束条件更容在坐标轴的顶点上与等值线产生交点. 圆形则是更容易在除坐标轴以外的地方产生相切点.&lt;/p&gt;

&lt;p&gt;由于上述的特征, Lasso算法可以通过调整正则化参数, 使得一些特征量的权重参数为 0, 也就是删除这些特征量 (这也会使得计算量会变小). &lt;strong&gt;因此Lasso回归也能够用来做特征选择.&lt;/strong&gt;  Lasso名字中的 Selection Operator 也指出了这一点. 另外和岭回归一样, 通过交叉验证, 选择出一个合适的正则化参数也是非常重要的.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;以上就是线性模型中常用的基本的三个算法. 写的内容不是很难, 但是在写的过程中我发现文章当中”统计模型” 以及 “机器学习” 的内容似乎混杂在了一起. 这不禁让我对这两者的区别产生了思考, 所以这篇文章花了相当长的时间. 估计之后会写一篇新的文章, 以总结自己针对两者的区别的调查和思考.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;这篇文章暂时不涉及求取最小值的方法, 多是采用梯度下降法. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sat, 17 Jul 2021 16:36:31 +0900</pubDate>
        <link>https://simcookies.github.io/2021/07/17/summary-of-machine-learning-algorithms-linear-model-1</link>
        <guid isPermaLink="true">https://simcookies.github.io/2021/07/17/summary-of-machine-learning-algorithms-linear-model-1</guid>
        
        <category>algorithm</category>
        
        <category>formula</category>
        
        
        <category>machine learning</category>
        
      </item>
    
      <item>
        <title>机器学习算法系列 - 逻辑回归</title>
        <description>&lt;p&gt;逻辑回归(Logistic Regression)也是非常经典以及常用的算法. 虽然名字叫做”回归”, 但实际上是一个常用于解决分类且多是二分类问题的算法. 在一些文献中也会被称为对数回归或者最大熵分类. 在这个模型中, 使用了逻辑函数(Logit)对描述单个实验可能结果的&lt;strong&gt;概率&lt;/strong&gt;进行建模.&lt;/p&gt;

&lt;h1 id=&quot;逻辑回归&quot;&gt;逻辑回归&lt;/h1&gt;

&lt;h2 id=&quot;logit-函数和-sigmoid-函数&quot;&gt;Logit 函数和 Sigmoid 函数&lt;/h2&gt;

&lt;p&gt;为了讨论逻辑回归中的核心也就是Logit函数, 首先要看Odds. Odds 又叫发生比或者叫几率, 指的是一个事件发生和不发生的概率的比值.&lt;/p&gt;

\[\text{odds}=\frac{p}{1-p}\notag\]

&lt;p&gt;如果能够预测出这个Odds, 也就能够计算得到该事件发生的概率了. 而Logit函数就是对这里的 Odds 取对数.&lt;/p&gt;

\[\text{logit}(\text{p})=\log\frac{p}{1-p}\notag\]

&lt;p&gt;&lt;strong&gt;问题假设&lt;/strong&gt;: 现在要预测能否通过银行的贷款审核(结果只有是否两种, 属于二分类问题), 而手头已有的数据特征量为用户的个人信息以及信用卡偿还信息等. (这里的特征量用 $x_i(i=1,2,\dots,n)$ 表示) 这里我们假设 $\text{logit}(\text{p})$ 和特征量之间服从线性关系 (至于为什么要这么假设, 到后面可以看到这个假设能够帮助我们做分类).&lt;/p&gt;

\[\begin{align*}
&amp;amp;\log\frac{p}{1-p}=\theta_0+\theta_1x_1+\theta_2x_2+\dots+\theta_nx_n=\vec{\theta}\cdot\vec{x}\\
&amp;amp;\Rightarrow \frac{p}{1-p} = \exp(\vec{\theta}\cdot\vec{x})\\
&amp;amp;\Rightarrow p=\frac{\exp(\vec{\theta}\cdot\vec{x})}{1+\exp(\vec{\theta}\cdot\vec{x})}=\frac{1}{1+\exp(-\vec{\theta}\cdot\vec{x})}=g(\vec{\theta}\cdot\vec{x})
\end{align*}\]

&lt;p&gt;最后得到的这个假设函数称之为 Logistic 函数, 也叫 &lt;strong&gt;Sigmoid 函数&lt;/strong&gt;.其函数图像为:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20210502173103.png&quot; alt=&quot;sigmoid&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其横轴就是特征量的权重和, 纵轴就是该事件发生的概率. 再根据得到的概率, 设定一个阈值, 也叫决策边界(Decision Bound). 比如这里我们设置阈值为0.5, 概率大于0.5的认为该事件会发生, 也就是银行贷款能通过, 小于0.5则不能通过. 也就是说, 不同于决策树分类得到的结果, 逻辑回归不仅可以得到最终的分类结果, 还能够得到中间产物 – 属于某一个类别的概率 $p$.&lt;/p&gt;

&lt;p&gt;在日常生活中, 我们会用到加权求和分类的方法. 给用户每个特征量一个权重之后, 计算加权求和. 如果分数超过了某一个阈值我们就认为可以放贷. 其实这个方法用类似上述的数学语言表达的话, 可以认为最后计算的总分减去阈值是真正的权重和. 这个特征量的权重和超过零的话就判断为该事件发生, 反之为不发生.这里也用到了一个假设函数, 就是&lt;strong&gt;阶梯函数&lt;/strong&gt;(Step function).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20210502175321.png&quot; alt=&quot;sigmoid_and_step&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如上图所示, 阶梯函数更加直接. 但是阶梯函数在加权和接近于0的部分会非常的敏感, 对数据的误差容忍度很低, 并且其在0处不能够微分. 相比之下, Sigmoid函数就显得温和一些, 且单调递增可微.&lt;/p&gt;

&lt;h2 id=&quot;逻辑回归的算法描述&quot;&gt;逻辑回归的算法描述&lt;/h2&gt;

&lt;p&gt;有了 Sigmoid 函数的加持, 从特征量 $\vec{x}$ 到对数Odds $\text{logit}(\text{p})$ 的一一映射就产生了, 问题就在于如何确定式子里的参数 $\theta$ 的值. 其实逻辑回归本质上是在用简单线性回归的预测结果去逼近对数发生比或者说对数几率, 所以在&lt;strong&gt;机器学习方法&lt;/strong&gt;这本书中, 周老师更愿意把 Logistic Regression 翻译成”对数几率回归”, 简称”对率回归”. 个人感觉非常赞同! (但是基于很多人还是会用”逻辑回归”这个词, 所以本文也保持这个名称)&lt;/p&gt;

&lt;p&gt;同线性回归一样, 为了确定参数 $\theta$ 需要定义一个代价函数, 再通过优化算法最小化代价函数. 首先假设函数(Hypothesis)为:&lt;/p&gt;

\[h_\theta(x)=\text{Sigmoid}(\theta^Tx)=\frac{1}{1+\exp(-\theta^Tx)}\notag\]

&lt;p&gt;需要进行学习的目标参数(Parameter)为上式中的 $\theta$. 线性回归中采用的代价函数为:&lt;/p&gt;

\[J(\theta)=\frac 1 m\sum_{i=1}^{m}\frac 1 2\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)^2\notag\]

&lt;p&gt;其中 $m$ 是数据的数量. 但是假设函数是 Logit 函数, 所以得到的代价函数将是非凸函数. 对于非凸函数使用梯度下降法将有可能找不到全局最小值. 这里采用对数极大似然估计可以将代价函数转换为凸函数:&lt;/p&gt;

\[J(\theta)=\frac 1 m \sum_{i=1}^m\left[-y^{(i)}\log\left(h_\theta\left(x^{(i)}\right)\right)-\left(1-y^{(i)}\right)\log\left(1-h_\theta\left(x^{(i)}\right)\right)\right]\notag\]

&lt;p&gt;可以求得代价函数对 $\theta$ 的导数为:&lt;/p&gt;

\[\frac\partial{\partial\theta_j}J(\theta)=\frac 1 m\sum_{i=1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)x_j^{(i)}\notag\]

&lt;p&gt;利用上述的导数不断地更新参数 $\theta$, 就能获得最小的代价函数 $\min_\theta J(\theta)$:&lt;/p&gt;

\[\theta_j\Leftarrow\theta_j-\alpha\frac\partial{\partial\theta_j}J(\theta)\notag\]

&lt;p&gt;这里的 $\alpha$ 为学习速率. 有了上面的值, 算法可以被描述为:&lt;/p&gt;

&lt;p&gt;输入: 训练数据集 D, 学习速率 $\alpha$&lt;br /&gt;
输出: 模型权重参数 $\theta$&lt;br /&gt;
步骤:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;计算代价函数平均梯度;&lt;/li&gt;
  &lt;li&gt;乘上学习速率;&lt;/li&gt;
  &lt;li&gt;计算更新权重参数 $\theta$;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;基于-python-scratch-的实现&quot;&gt;基于 Python Scratch 的实现&lt;/h1&gt;

&lt;p&gt;为了方便理解, 可以使用 Scikit-Learn 的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;make_classification&lt;/code&gt; 帮助我们生成一堆用来分类的, 简单的数据.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_classification&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_classification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_informative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_redundant&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_clusters_per_class&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;class_sep&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这里生成了20组拥有两个类别的数据, 特征量的数量也只有1个. 绘制成图形如下图:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20210509124118.png&quot; alt=&quot;下载&quot; /&gt;&lt;/p&gt;

&lt;p&gt;首先定义代价函数:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 后续要用的 sigmoid 函数
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;# 正向计算得到预测值
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ravel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;hx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hx&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;# 代价函数
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cost_fuction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;class1_cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;class2_cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class1_cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class2_cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;再定义用于更新权重参数的核心函数:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;update_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这里默认学习速率 lr 为 0.01. 最后定义包含核心算法的, 用于学习的函数:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cost_history&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost_fuction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cost_history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;update_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;Epochs:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iters&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, iters: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, cost: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost_history&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;训练函数中的迭代次数不限, 而是用 Epoch 来指定. 每一个 Epoch 里面默认循环 iter = 1000 次, 并且输出每个Epoch的代价函数值, 以查看训练过程. 预先使用随机函数定义好初始的权重参数, 使用默认10次Epoch来训练.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;initial_weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost_history&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initial_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;上述代码的输出为:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Epochs:0, iters: 0, cost: 0.32268668797387234
Epochs:1, iters: 1000, cost: 0.1632418968511627
Epochs:2, iters: 2000, cost: 0.13241899314581226
Epochs:3, iters: 3000, cost: 0.11868723129920973
Epochs:4, iters: 4000, cost: 0.1108135413966402
Epochs:5, iters: 5000, cost: 0.10568263401138037
Epochs:6, iters: 6000, cost: 0.10206687727097812
Epochs:7, iters: 7000, cost: 0.09937998342713258
Epochs:8, iters: 8000, cost: 0.09730508677348992
Epochs:9, iters: 9000, cost: 0.09565536684950605
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;可以看到, 随着循环次数的增加, 代价函数的值再不断地变小. 可视化一下看一下:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20210509152656.png&quot; alt=&quot;下载 (1)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看出循环了10000次也就是10个Epoch的时候, 代价函数的值已经很小了, 就是说对于学习速率 0.01 的情况, 学习到这里就可以了. 尝试不同的学习速率:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20210509153535.png&quot; alt=&quot;下载&quot; /&gt;&lt;/p&gt;

&lt;p&gt;很明显的看到, 学习速率越大, 代价函数的值下降的也越快, 至少在这个例子里是这样. 对于复杂的问题的话, 合适的学习速率的选择是非常重要的. 对于 0.01 的学习速率, 得到其计算好的权重参数后, 考察一下其模型的精度. 还是用最直观的描点看:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20210509153941.png&quot; alt=&quot;下载&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对比预测的点还有实际的点, 可以看到20点中有一个点(十分接近于0)预测发生了错位(精度为95%), 虽然可以通过继续学习提高精度, 但是基本上已经算可以了. 复杂的问题中不断地学习会导致过学习问题, 这也是很常见的.&lt;/p&gt;

&lt;h1 id=&quot;基于-scikit-learn-的实现&quot;&gt;基于 Scikit-Learn 的实现&lt;/h1&gt;

&lt;p&gt;Scikit-Learn中提供了 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LogisticRegression&lt;/code&gt; 的函数. 至于数据我们采用 Scikit-Learn 的 Toy Datasets 中的判断乳腺癌阴阳性的数据集. 并且为了便于计算, 使用最大最小值 Scaler 将特征量映射到 (0, 1) 区间上去.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_breast_cancer&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MinMaxScaler&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_breast_cancer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;normalized_range&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MinMaxScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_range&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normalized_range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogisticRegression&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogisticRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# =&amp;gt; 0.9718804920913884
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;就是这样的简单, 基本上默认的参数可以保证很高的精度, 达到了0.97以上.再尝试用我们自己的函数:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;initial_weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initial_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# =&amp;gt; 0.9736379613356766
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在使用很大的学习速率还有足够的迭代次数之后, 也能达到较高的精度. (当然这里还没有用到交叉验证, 用了之后我们的模型应该过学习的很厉害. 这一点这篇文章里暂时不考虑.)&lt;/p&gt;

&lt;p&gt;上述代码的原版在这里&lt;a href=&quot;https://github.com/simcookies/algorithm_implement/blob/master/ml/Logistic_Regression_scratch.ipynb&quot;&gt;Github&lt;/a&gt;, 以上就是对逻辑回归算法的一的简单总结.&lt;/p&gt;
</description>
        <pubDate>Sun, 09 May 2021 16:36:31 +0900</pubDate>
        <link>https://simcookies.github.io/2021/05/09/summary-of-machine-learning-algorithms-logistic-regression</link>
        <guid isPermaLink="true">https://simcookies.github.io/2021/05/09/summary-of-machine-learning-algorithms-logistic-regression</guid>
        
        <category>algorithm</category>
        
        <category>formula</category>
        
        
        <category>machine learning</category>
        
      </item>
    
      <item>
        <title>机器学习算法系列 - 决策树C4.5算法</title>
        <description>&lt;p&gt;上一篇文章&lt;a href=&quot;/2020/11/12/summary-of-machine-learning-algorithms-decision-tree-id3&quot;&gt;机器学习算法系列 - 决策树ID3算法&lt;/a&gt;, 对决策树中非常典型的 ID3 算法做了深入介绍. 这篇文章本来会大概总结一下 C4.5 算法. 但是由于 C4.5 算法与 ID3 算法高度的一致, 所以这里只谈及一些前者相对于后者的改进.&lt;/p&gt;

&lt;h1 id=&quot;提出问题&quot;&gt;提出问题&lt;/h1&gt;

&lt;p&gt;ID3 算法的核心在于找到信息增益最大的特征, 在上一篇文章的例子当中, 就能感觉到其非常直观和易于理解. 但是在一些特殊的情况下, 单是绝对的信息增益是不能够找到真正最有用的特征的.&lt;/p&gt;

&lt;p&gt;假设训练数据集中有一列 ID, 每条记录都有一个独立的 ID. 那么这个 ID 列的条件熵会变成 0 (因为在 ID 确定的条件下, 出现的类别数只有一个). 使用这个值为 0 的条件熵得到的信息增益就会非常大 (等于原来的信息熵). 从现实意义上看, 这也是符合的. 因为一条记录如果确定了它的 ID, 那么他的类别一定也是能被确定的. 原因就是 ID 这个信息里面包含了太多的信息量了. 但是显然不能使用 ID 作为一个节点的特征. 实际上, 对于某些特征来说, 其每个属性所包含的每个分类类别很多的时候, 都会得到很大信息增益. 这个时候只通过信息增益是不能选出有效的特征的.&lt;/p&gt;

&lt;p&gt;概括地说, 就是一个特征其特征熵很大的时候信息增益就很大, 反之就很小.&lt;/p&gt;

&lt;h1 id=&quot;解决方法&quot;&gt;解决方法&lt;/h1&gt;

&lt;p&gt;为了排除特征本身信息熵地干扰, C4.5 算法选择了信息增益率 (Gain Ratio) 作为选择分支的准则。对于特征熵很大的特征，除上一个相应的较大的数值，反之特征熵小的特征除一个较小的数值，使得特征的信息熵的影响变小。这里 C4.5 算法中所除以的数值使用了属性熵。&lt;/p&gt;

\[H_A(D) = -\sum\limits_{i=1}^{n}\frac{|D_i|}{|D|}\log\frac{|D_i|}{|D|}\notag\]

&lt;p&gt;当数据集 $D$ 中 $A$ 属性里有很多不同的取值时，上述的属性熵就会变得很大。这样由信息增益除以该值所得到的增益率就能够正真反映出属性 $A$ 作为一个特征，区分样本的能力。&lt;strong&gt;最终就避免了 ID3 趋向与选择取值较多的属性作为节点的问题&lt;/strong&gt;。&lt;/p&gt;

&lt;h2 id=&quot;描述&quot;&gt;描述&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;从根节点开始, 对节点计算所有可能的特征的信息增益;&lt;/li&gt;
  &lt;li&gt;选择&lt;strong&gt;信息增益比&lt;/strong&gt;最大的特征作为节点的特征, 构建子节点;&lt;/li&gt;
  &lt;li&gt;对子节点递归调用上述方法, 构建决策树;&lt;/li&gt;
  &lt;li&gt;直到所有特征的信息增益均很小 (可用事前决定的阈值进行判断) 或者没有特征可以选择&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;步骤&quot;&gt;步骤&lt;/h2&gt;

&lt;p&gt;输入: 训练数据集 $D$, 特征集 $A$, 阈值 $\epsilon$&lt;br /&gt;
输出: 决策树 $T$&lt;br /&gt;
步骤:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;若 $D$ 中所有实例输入同一类 $C_k$, 则 $T$ 为单节点树, 并将类 $C_k$ 作为该节点的类标记, 返回 $T$; (只有一个类的情况)&lt;/li&gt;
  &lt;li&gt;若 $A=\varnothing$ , 则 $T$ 为单节点树, 并将 $D$ 实例数最大的类 $C_k$ 作为该节点的类标记, 返回 $T$; (没有特征量的情况)&lt;/li&gt;
  &lt;li&gt;否则, 计算 $A$ 中各个特征对于 $D$ 的&lt;strong&gt;信息增益比&lt;/strong&gt;, 选择信息增益比最大的特征 $A_g$;&lt;/li&gt;
  &lt;li&gt;如果 $A_g$ 的信息增益比小于阈值 $\epsilon$, 则置 $T$ 为单节点树, 并将 $D$ 中实例数最大的类 $C_k$ 作为该节点的类标记, 返回 $T$;&lt;/li&gt;
  &lt;li&gt;否则, 对 $A_g$ 的每一个可能值 $a_i$, 依 $A_g = a_i$ 将 $D$ 分割为若干个非空子集 $D_i$, 将 $D_i$ 中实例数最大的类作为标记, 构建子节点, 由节点及其子节点构成树 $T$, 返回 $T$;&lt;/li&gt;
  &lt;li&gt;对于第 $i$ 个子节点, 以 $D_i$ 为训练集, 以 $A-{A_g}$ 为特征集, 递归调用步1 ~ 步5, 得到子树 $T_i$, 返回 $T_i$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;可以看出 C4.5 算法和 ID3 算法除了特征选择的参照物不同之外，其他基本上是一致的。&lt;/p&gt;

&lt;h1 id=&quot;基于-python-scratch-的实现&quot;&gt;基于 Python Scratch 的实现&lt;/h1&gt;

&lt;p&gt;对于同一个问题&lt;a href=&quot;/2020/11/12/summary-of-machine-learning-algorithms-decision-tree-id3#问题提出&quot;&gt;预测能否贷款&lt;/a&gt;，和前文的ID3一样，这里也给出 Python Scratch 的实现。代码上C4.5算法和ID3算法也是一致的，只是增加了计算属性熵的部分，并且原来取得最大信息增益的函数需要改为计算最大信息增益率的函数。&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;find_best_gain_ratio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h_d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;calc_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;gr_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;feature_gr_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x_feature&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;calc_probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_feature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Hs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;calc_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_feature&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_feature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;h_da&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Hs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h_d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h_da&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# 计算属性熵
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;h_ad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;calc_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_feature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;gr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h_ad&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gr_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;gr_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gr&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;feature_gr_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gr_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feature_gr_max&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;最终版的代码在这里 &lt;a href=&quot;https://github.com/simcookies/algorithm_implement/blob/master/ml/Decision_Tree_C4.5_scratch.ipynb&quot;&gt;Github&lt;/a&gt;。scikit-learn 的实现和之前的一篇完全一样，这里不再赘述。另外根据 scikit-learn 的官方文档，决策树中采用的并不是 ID3/C4.5 算法，而是 CART。（这就很尴尬😅了，不过知道算法的实现终归是好事。）&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;scikit-learn uses an optimised version of the CART algorithm; however, scikit-learn implementation does not support categorical variables for now.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;以上就是关于 C4.5 算法的说明。之后算法的发明者 Ross Quinlan 将算法更新到了 C5.0（有专利保护），使得计算更快，结果更精确，不过这就是另外的话了。&lt;/p&gt;
</description>
        <pubDate>Sat, 13 Mar 2021 12:30:31 +0900</pubDate>
        <link>https://simcookies.github.io/2021/03/13/summary-of-machine-learning-algorithms-decision-tree-c4.5</link>
        <guid isPermaLink="true">https://simcookies.github.io/2021/03/13/summary-of-machine-learning-algorithms-decision-tree-c4.5</guid>
        
        <category>algorithm</category>
        
        <category>formula</category>
        
        
        <category>machine learning</category>
        
      </item>
    
      <item>
        <title>机器学习算法系列 - 决策树ID3算法</title>
        <description>&lt;p&gt;在上一篇文章&amp;lt;&lt;a href=&quot;/2020/11/02/summary-of-machine-learning-algorithms-decision-tree&quot;&gt;机器学习算法系列 - 决策树(序)&lt;/a&gt;&amp;gt;中介绍了三个决策学习的算法,  ID3, C4.5, CART. 这一篇文章对 ID3 作深一点的介绍, 并且基于 Scratch Python 以及 Scikit-learn 包进行实现.&lt;/p&gt;

&lt;h1 id=&quot;id3-的再介绍&quot;&gt;ID3 的再介绍&lt;/h1&gt;

&lt;p&gt;ID3 全称是 Iterative Dichotomiser 3 (迭代二叉树 3 代), 是由 &lt;a href=&quot;https://en.wikipedia.org/wiki/Ross_Quinlan&quot;&gt;Ross Quinlan&lt;/a&gt; 发明的用于决策树的算法. 该算法基于信息熵和信息增益的概念, 能够生成用于分类的决策树. 其算法的语言描述为:&lt;/p&gt;

&lt;h2 id=&quot;描述&quot;&gt;描述&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;从根节点开始, 对节点计算所有可能的特征的信息增益;&lt;/li&gt;
  &lt;li&gt;选择信息增益最大的特征作为节点的特征, 构建子节点;&lt;/li&gt;
  &lt;li&gt;对子节点递归调用上述方法, 构建决策树;&lt;/li&gt;
  &lt;li&gt;直到所有特征的信息增益均很小 (可用事前决定的阈值进行判断) 或者没有特征可以选择&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;步骤&quot;&gt;步骤&lt;/h2&gt;

&lt;p&gt;输入: 训练数据集 $D$, 特征集 $A$, 阈值 $\epsilon$&lt;br /&gt;
输出: 决策树 $T$&lt;br /&gt;
步骤:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;若 $D$ 中所有实例输入同一类 $C_k$, 则 $T$ 为单节点树, 并将类 $C_k$ 作为该节点的类标记, 返回 $T$; (只有一个类的情况)&lt;/li&gt;
  &lt;li&gt;若 $A=\varnothing$ , 则 $T$ 为单节点树, 并将 $D$ 实例数最大的类 $C_k$ 作为该节点的类标记, 返回 $T$; (没有特征量的情况)&lt;/li&gt;
  &lt;li&gt;否则, 计算 $A$ 中各个特征对于 $D$ 的信息增益, 选择信息增益最大的特征 $A_g$;&lt;/li&gt;
  &lt;li&gt;如果 $A_g$ 的信息增益小于阈值 $\epsilon$, 则置 $T$ 为单节点树, 并将 $D$ 中实例数最大的类 $C_k$ 作为该节点的类标记, 返回 $T$;&lt;/li&gt;
  &lt;li&gt;否则, 对 $A_g$ 的每一个可能值 $a_i$, 依 $A_g = a_i$ 将 $D$ 分割为若干个非空子集 $D_i$, 将 $D_i$ 中实例数最大的类作为标记, 构建子节点, 由节点及其子节点构成树 $T$, 返回 $T$;&lt;/li&gt;
  &lt;li&gt;对于第 $i$ 个子节点, 以 $D_i$ 为训练集, 以 $A-{A_g}$ 为特征集, 递归调用步1 ~ 步5, 得到子树 $T_i$, 返回 $T_i$.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;问题提出&quot;&gt;问题提出&lt;/h1&gt;

&lt;p&gt;通过一个人的年龄, 工作状况, 是否有房子以及信贷状况等信息预测他能否申请贷款. 数据如下:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;ID&lt;/th&gt;
      &lt;th&gt;年龄&lt;/th&gt;
      &lt;th&gt;有工作&lt;/th&gt;
      &lt;th&gt;有自己的房子&lt;/th&gt;
      &lt;th&gt;信贷状况&lt;/th&gt;
      &lt;th&gt;类别&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;青年&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;一般&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;青年&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;好&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;青年&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;好&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;青年&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
      &lt;td&gt;一般&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;青年&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;一般&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;中年&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;一般&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;中年&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;好&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;中年&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
      &lt;td&gt;好&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;中年&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
      &lt;td&gt;非常好&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;中年&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
      &lt;td&gt;非常好&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;老年&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
      &lt;td&gt;非常好&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;老年&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
      &lt;td&gt;好&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;老年&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;好&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;老年&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;非常好&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;老年&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;一般&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;使用 ID3 算法生成可用于分类的决策树.&lt;/p&gt;

&lt;h1 id=&quot;基于-python-scratch-的实现&quot;&gt;基于 Python Scratch 的实现&lt;/h1&gt;

&lt;p&gt;一共有 4 种特征量 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;年龄&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;工作状况&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;是否有房子&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;信贷状况&lt;/code&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;其实不能算得上真正的 Scratch, 毕竟用了 Numpy :sweat_smile:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;feature_columns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;年龄&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;是否有工作&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;是否有自己的房子&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;信贷状况&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;青年&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;一般&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;青年&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;好&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;青年&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;是&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;好&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;是&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;青年&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;是&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;是&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;一般&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;是&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;青年&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;一般&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;中年&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;一般&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;中年&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;好&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;中年&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;是&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;是&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;好&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;是&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;中年&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;是&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;非常好&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;是&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;中年&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;是&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;非常好&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;是&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;老年&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;是&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;非常好&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;是&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;老年&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;是&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;好&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;是&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;老年&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;是&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;好&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;是&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;老年&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;是&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;非常好&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;是&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;老年&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;一般&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;有两个函数需要经常用到, 可以事先定义好.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 计算 data 数据中, 各个类别出现的概率
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;calc_probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;probs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;probs&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 计算 data 的信息熵
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;calc_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_ps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;calc_probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;entropy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_ps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;entropy&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;直接对 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y_train&lt;/code&gt; 计算熵 $H(D)$:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;H_D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;calc_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# =&amp;gt; 0.9709505944546686
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;计算并且选择出拥有最大信息增益的特征:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;feature_g_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x_feature&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;calc_probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_feature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Hs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;calc_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_feature&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_feature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;H_DA&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Hs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H_D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H_DA&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;g_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;feature_g_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;最大信息增益: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g_max&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, 对应特征: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_g_max&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# =&amp;gt; 最大信息增益: 0.4199730940219749, 对应特征: 2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这样第一个循环下来, 就得到了根节点的特征为 2, 也就是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;是否有自己的房子&lt;/code&gt; 这个特征. 这样决策树就变成了这样:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;graph TD
root[是否有自己的房子?]-- 没有 --&amp;gt;N1[ID: 1,2,3,5,6,7,13,14,15]
root-- 有 --&amp;gt;N2[ID: 4,8,9,10,11,12]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;确定了根节点后, 就得到了两个子节点. 就可以再次进入循环确定节点的特征. 比如对于上图中左边的节点, 需要考察的特征为 $A-{A_g}$ 也就是除了房子状况之外的3个特征.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 更新数据
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feature_g_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;否&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;H_D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;calc_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 去掉已经用掉的特征
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;remove&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_g_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;feature_g_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x_feature&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;calc_probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_feature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Hs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;calc_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_feature&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_feature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;H_DA&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Hs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H_D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H_DA&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;g_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;feature_g_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;最大信息增益: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g_max&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, 对应特征: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_g_max&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# =&amp;gt; 最大信息增益: 0.9182958340544896, 对应特征: 1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这样左边的节点就选择了特征 1 也就是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;是否有工作&lt;/code&gt; 作为其特征. 决策树就更新为:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;graph TD
root[是否有自己的房子?]-- 没有 --&amp;gt;N1[是否有工作?]
root-- 有 --&amp;gt;N2[ID: 4,8,9,10,11,12]
N1 -- 没有 --&amp;gt; N3[ID: 1,2,5,6,7,15]
N1 -- 有 --&amp;gt; N4[ID: 3,13,14]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;左边的节点可以继续迭代往下生长. 再来看右边的节点.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 更新数据
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feature_g_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# =&amp;gt; 1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这些 ID 对应的人只有一个类别, 即都是可以申请贷款的, 因此右节点确定为类标记 “1” 之后就结束了. 之后的决策树更新为:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;graph TD
root[是否有自己的房子?]-- 没有 --&amp;gt;N1[是否有工作?]
root-- 有 --&amp;gt;N2(可以申请贷款)
style N2 fill:#ff0
N1 -- 没有 --&amp;gt; N3[ID: 1,2,5,6,7,15]
N1 -- 有 --&amp;gt; N4[ID: 3,13,14]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上述的操作按照算法描述可以写在一个迭代函数中 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ID3_tree_generate(data, features, e=0.01)&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 计算并返回最大信息增益和特征
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;find_best_gain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;H_D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;calc_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;g_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;feature_g_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x_feature&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;calc_probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_feature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Hs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;calc_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_feature&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_feature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;H_DA&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Hs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H_D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H_DA&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;g_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;feature_g_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feature_g_max&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;# ID3 迭代算法
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;ID3_tree_generate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Step 1
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# 数据都是一个类别时
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Step 2
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# 特征的数量为0时
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bincount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Step 3
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# 计算最大信息增益和特征
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;g_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feature_g_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;find_best_gain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Step 4
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# 最大信息增益比阈值小时
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;tree&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_g_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bincount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()}&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tree&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Step 5, 6
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# 分类迭代生成子树
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;splited_datas&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feature_g_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tree&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_g_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;splited_datas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sub_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feature_g_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sub_features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sub_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;remove&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_g_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;tree&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_g_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ID3_tree_generate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;sub_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_features&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tree&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;最后, 直接调用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ID3_tree_generate&lt;/code&gt; 生成决策树.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;本例使用了 dict 存储树&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ID3_tree_generate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# =&amp;gt; {'是否有自己的房子': {'否': {'是否有工作': {'否': '否', '是': '是'}}, '是': '是'}}
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;最终的树就成了:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;graph TD
root[是否有自己的房子?]-- 没有 --&amp;gt;N1[是否有工作?]
root-- 有 --&amp;gt;N2(可以申请贷款)
style N2 fill:#ff0
N1 -- 没有 --&amp;gt; N3(不可以申请贷款)
style N3 fill:#ff0
N1 -- 有 --&amp;gt; N4(可以申请贷款)
style N4 fill:#ff0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样就完成了一个可用于预测的决策树模型了. 单从这个例子的数据看, 可以发现房子和工作才是能否贷款的最主要原因, 和年龄状况/信贷状况没有太大关系. 另外需要注意的是, 因为这里模型非常的简单, 所以没有用到树的修剪, 阈值的设置也没有起到什么作用.&lt;/p&gt;

&lt;h1 id=&quot;基于-scikit-learn-的实现&quot;&gt;基于 Scikit-Learn 的实现&lt;/h1&gt;

&lt;p&gt;scikit-learn 的 Tree 提供了 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DecisionTreeClassifier&lt;/code&gt; 用于决策学习. 使用同样的数据来实现.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# scikit-learn 需要数字进行处理, 当然也可以使用 OneHotEncoder
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.tree&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DecisionTreeClassifier&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DecisionTreeClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;entropy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这里特别指定了 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;criterion&lt;/code&gt; 为 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;entropy&lt;/code&gt; , 是因为 scikit-learn 默认采用了基尼指数作为指标. 可以使用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pydotplus&lt;/code&gt; 将训练好的树进行可视化.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;IPython.display&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pydotplus&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.tree&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;export_graphviz&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dot_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;export_graphviz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;out_file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;filled&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rounded&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;feature_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  
    &lt;span class=&quot;n&quot;&gt;class_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;能贷款&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;不能贷款&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;special_characters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pydotplus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;graph_from_dot_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create_png&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20201113191853.png&quot; alt=&quot;决策树&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看出 scikit-learn 得到的树和上面 scratch 得到树是一样的. 最终版的代码在这里 &lt;a href=&quot;https://github.com/simcookies/algorithm_implement/blob/master/ml/Decision_Tree_ID3_scratch.ipynb&quot;&gt;Github&lt;/a&gt;。到这里, 关于 ID3 简单的实现就结束了.&lt;/p&gt;
</description>
        <pubDate>Thu, 12 Nov 2020 22:02:31 +0900</pubDate>
        <link>https://simcookies.github.io/2020/11/12/summary-of-machine-learning-algorithms-decision-tree-id3</link>
        <guid isPermaLink="true">https://simcookies.github.io/2020/11/12/summary-of-machine-learning-algorithms-decision-tree-id3</guid>
        
        <category>algorithm</category>
        
        <category>formula</category>
        
        
        <category>machine learning</category>
        
      </item>
    
      <item>
        <title>机器学习算法系列 - 决策树(序)</title>
        <description>&lt;p&gt;决策树是机器学习当中一系列非常基本的算法, 包括了常见的 ID3, C4.5, CART等等. 决策树学习是根据数据的属性采用树状结构建立的一种决策模型, 并且可以用于解决&lt;strong&gt;分类&lt;/strong&gt;和&lt;strong&gt;回归&lt;/strong&gt;的问题&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. 相对于其他的机器学习算法相比, 决策树背后的数学复杂度并不是很高, 比较容易理解.&lt;/p&gt;

&lt;h1 id=&quot;什么是决策树&quot;&gt;什么是决策树&lt;/h1&gt;

&lt;p&gt;可以通过概念去理解决策树. 假设现在要做一个”是否要出去吃午饭”的决定. 用一个图来说明:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;graph TD
root[出去吃饭吗?]-- 到12点了 --&amp;gt;N1[去]
root-- 没到12点呢 --&amp;gt;N2[不去]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上就是一个非常基本的决策树. 如果加入一些更复杂的因素的话, 决策树也会变得更深. 这里假设”和小明一起去吃中华料理”, 图就可以变成这样:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;graph TD
root[出去吃饭吗?] -- 到12点了 --&amp;gt;N1[看小明]
root -- 没到12点呢 --&amp;gt;N2[不去了]
N1 -- 想出去吃 --&amp;gt; N3[看附近餐馆]
N1 -- 不想出去吃 --&amp;gt; N4[不去了]
N3 -- 有中华 --&amp;gt; N5[出去吃]
N3 -- 没有中华 --&amp;gt; N6[不吃了]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从树的根部开始, 依照当前状况选择路线, 直到做出决定. 因此称之为决策树.&lt;/p&gt;

&lt;p&gt;从定义上看, 决策树是一种树状结构, 自然有树状结构的基本单位, 也就是&lt;strong&gt;节点(Node)&lt;/strong&gt;. 而节点与节点之间的连接称为&lt;strong&gt;分支 (branch)&lt;/strong&gt;. 结构的开端称为&lt;strong&gt;根 (root)&lt;/strong&gt;. 根节点之外的节点称为&lt;strong&gt;子节点 (child)&lt;/strong&gt;, 其中特殊的, 没有连接到子节点的节点称为&lt;strong&gt;叶节点 (leaf)&lt;/strong&gt;. 绘制出来就是:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;graph TD
Root --&amp;gt; child1[Child]
Root --&amp;gt; child2[Leaf]
child1 --&amp;gt; child3[Leaf]
child1 --&amp;gt; child4[Leaf]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个树状结构是通过训练得到的, 而不是人为生成的. 并且决策树模型非常易于实现, 可解释性强, 也符合人的逻辑. 在决策学习的过程中, 经常会使用 Graphviz, matplotlib 加以可视化(上述的图是用 mermaid画的), 并进行加深理解以便于调整模型.&lt;/p&gt;

&lt;h1 id=&quot;决策学习的过程&quot;&gt;决策学习的过程&lt;/h1&gt;

&lt;p&gt;决策学习包括了下面三个步骤：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;特征选择&lt;/li&gt;
  &lt;li&gt;决策树生成&lt;/li&gt;
  &lt;li&gt;决策树修剪&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;特征选择&quot;&gt;特征选择&lt;/h2&gt;

&lt;p&gt;决策学习的特征选择需要决定使用哪些特征量来做判断, 也就是什么样的特征量可以用来作为树的节点以及节点的排列顺序. 训练数据集中, 属性的个数有很多, 不同的属性用于区分数据的作用有大有小. 因此特征选择的重点就在于, 找到和分类结果相关性较高的特征, 也就是分类能力强的特征.&lt;/p&gt;

&lt;p&gt;这些特征能够尽可能地分开数据, 分开的数据尽量的规整(可以说是更&lt;strong&gt;纯&lt;/strong&gt;). 一言以蔽之, &lt;strong&gt;将原本无序的数据变得更加有序.&lt;/strong&gt; 常用到的衡量指标有三个:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;信息增益 (Information Gain)&lt;/li&gt;
  &lt;li&gt;增益比率 (Gain Ratio)&lt;/li&gt;
  &lt;li&gt;基尼指数 (Gini Index)&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;信息增益-information-gain&quot;&gt;信息增益 (Information Gain)&lt;/h3&gt;

&lt;p&gt;上面说了特征选择的重点在于找到分类能力强的特征量. 分类能力强指, 知道了一个特征的属性之后, 就可以最大程度上做出决定了.&lt;/p&gt;

&lt;p&gt;用上述的吃饭的例子来说, “现在的时刻”这个特征量一旦确定它是”是12点前”这个属性, 就可以做出”不去吃饭”这个决定了. &lt;strong&gt;“现在的时刻”这个特征量含有了最丰富的信息量, 知道了它的内容, 离做决定就差不多了.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;那么在信息论中, 什么东西可以量化原本数据的信息量呢? 答案就是&lt;strong&gt;熵 (Entropy)&lt;/strong&gt;. 在”现在的时刻是12点前”这个条件之下信息量是&lt;strong&gt;条件熵 (Conditional Entropy)&lt;/strong&gt;. 而&lt;strong&gt;信息增益 = 熵 - 条件熵&lt;/strong&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;信息增益大就表示, 这个特征量一旦属性确定之后, 能够消除掉很大的不确定性, 说明该特征量越重要.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这样特征选择的问题就变成了”&lt;strong&gt;找到信息增益大的特征量&lt;/strong&gt;“了.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(可以跳过)&lt;/em&gt; 来看数学上的定义:&lt;/p&gt;

&lt;p&gt;假设 $X$ 是一个取有限个值的离散随机变量, 其概率分布为&lt;/p&gt;

\[P(X=x_i) = p_i\qquad i=1,2,\dots,n\notag\]

&lt;p&gt;则随机变量 $X$ 的熵定义为&lt;/p&gt;

\[H(X)=-\sum_{i=1}^np_i\log p_i\notag\]

&lt;p&gt;熵越大代表随机变量的不确定性就越大.&lt;/p&gt;

&lt;p&gt;设有随机变量 $(X, Y)$ , 其联合概率分布为&lt;/p&gt;

\[P(X=x_i, Y=y_j) = p_{ij}\qquad i=1,2,\dots,n; j=1,2,\cdots,m\notag\]

&lt;p&gt;条件熵 $H(Y|X)$ 表示在已知随机变量 $X$ 的条件下随机变量 $Y$ 的不确定性， 定义为&lt;/p&gt;

\[H(Y|X)=\sum_{i=1}^n p_iH(Y|X=x_1)\qquad (p_i=P(X=x_i), i=1,2,\dots,n)\notag\]

&lt;p&gt;实际计算中, 上述的两个熵都是由数据估计(最大似然估计)出来的, 因此都是经验熵. 在这两个值都得到的基础上, 可以定义信息增益. 特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D, A)$ 为集合 $D$ 的经验熵 $H(D)$ 与特征 $A$ 给定条件下 $D$ 的经验条件熵 $H(D|A)$ 之差, 即&lt;/p&gt;

\[g(D,A)=H(D)-H(D|A)\notag\]

&lt;h3 id=&quot;增益比率-gain-ratio&quot;&gt;增益比率 (Gain Ratio)&lt;/h3&gt;

&lt;p&gt;如果只是用上述的信息增益的话, 会产生一个问题, 信息增益的大小是相对于一个特定的训练数据集和特征量的. 如果熵 $H(D)$ 本身就很大, 信息增益也会很大. 例如有一个ID的特征量, 因为ID本身就是独一无二的, 一旦确定之后决策也就确定了, 单看信息增益会得到很大的值. 而原因是拥有ID的数据本来熵就很大 (就是 $H(D)$ 很大), 利用这个特征分类出来的子集的信息熵基本为0, 因此计算出来的信息增益自然很大. 但是显然这是没有意义的特征; 另外, 还有类别取值较多的特征比取值较少的特征信息增益大等问题.&lt;/p&gt;

&lt;p&gt;因此, 需要一个相对的值来进行校正, 这就导入了&lt;strong&gt;信息增益比&lt;/strong&gt;:&lt;/p&gt;

\[g_R(D,A)=\frac{g(D,A)}{H_A(D)}\notag\]

&lt;p&gt;$H_A(D)$ 为特征 $A$ 的属性熵, 也叫特征熵:&lt;/p&gt;

\[H_A(D) = -\sum\limits_{i=1}^{n}\frac{|D_i|}{|D|}\log\frac{|D_i|}{|D|}\notag\]

&lt;h3 id=&quot;基尼指数-gini-index&quot;&gt;基尼指数 (Gini Index)&lt;/h3&gt;

&lt;p&gt;上面的信息增益或者是增益比率中涉及到信息熵的运算, 因此含有大量的对数运算. 为了简化运算同时保留信息熵的特点, 引入了基尼指数 (Gini Index) 也叫基尼不纯度. 基尼不纯度的定义为&lt;/p&gt;

\[Gini(p) = \sum_{k=1}^{K}p_k(1-p_k) = 1-\sum_{k=1}^{K}p_k^2\notag\]

&lt;p&gt;$k$ 为数据集 $D$ 中类别的个数, $p_k$ 表示某种类型出现的概率. 从式子本身的意义来看, 反映了从 $D$ 中随机抽取两个样本, 其类别不一致的概率. 因此, 这个值越小表示数据的纯度越高 (不纯度越低). 数据集 $D$ 的基尼指数为&lt;/p&gt;

\[Gini(D) = 1- \sum_{k=1}^K \left(\frac{|C_k|}{|D|}\right)^2\notag\]

&lt;p&gt;而对于一个特定的特征 $A$, 假设其可以分为 $D_1$ 和 $D_2$ 两个子集, 则其基尼指数为&lt;/p&gt;

\[Gini(D, A)=\frac{|D_1|}{|D|}Gini(|D_1|) + \frac{|D_2|}{|D|}Gini(|D_2|)\notag\]

&lt;p&gt;在特征选取中, 和信息增益或者增益比不同的是, 优先选择基尼指数&lt;strong&gt;小&lt;/strong&gt;的特征. 这个值被CART算法用于生成决策树, 既可以作为分类也可以作为回归的模型.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;决策树生成&quot;&gt;决策树生成&lt;/h2&gt;

&lt;p&gt;有了上述的指标之后, 可以开始生成决策树. 以信息增益作为例子来看, 从根节点开始, 首先选择信息增益最大的特征作为节点特征, 然后根据特征的不同取值建立子节点. 然后再对于每个子节点进行相同的处理 (计算最大信息增益 ⇒ 建立子节点), 直到最终特征的信息增益很小 (这里需要设置一个信息增益的阈值, 以防止树变得太过于复杂) 或者没有特征可以选择为止.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;具体的构建决策树的算法有 ID3, C4.5 和 CART.&lt;/p&gt;

&lt;h3 id=&quot;id3-算法&quot;&gt;ID3 算法&lt;/h3&gt;

&lt;p&gt;输入: 训练数据集 $D$, 特征集 $A$, 阈值 $\epsilon$&lt;br /&gt;
输出: 决策树 $T$&lt;br /&gt;
步骤:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;若 $D$ 中所有实例输入同一类 $C_k$, 则 $T$ 为单节点树, 并将类 $C_k$ 作为该节点的类标记, 返回 $T$; (只有一个类的情况)&lt;/li&gt;
  &lt;li&gt;若 $A=\varnothing$ , 则 $T$ 为单节点树, 并将 $D$ 实例数最大的类 $C_k$ 作为该节点的类标记, 返回 $T$; (没有特征量的情况)&lt;/li&gt;
  &lt;li&gt;否则, 计算 $A$ 中各个特征对于 $D$ 的信息增益, 选择信息增益最大的特征 $A_g$;&lt;/li&gt;
  &lt;li&gt;如果 $A_g$ 的信息增益小于阈值 $\epsilon$, 则置 $T$ 为单节点树, 并将 $D$ 中实例数最大的类 $C_k$ 作为该节点的类标记, 返回 $T$;&lt;/li&gt;
  &lt;li&gt;否则, 对 $A_g$ 的每一个可能值 $a_i$, 依 $A_g = a_i$ 将 $D$ 分割为若干个非空子集 $D_i$, 将 $D_i$ 中实例数最大的类作为标记, 构建子节点, 由节点及其子节点构成树 $T$, 返回 $T$;&lt;/li&gt;
  &lt;li&gt;对于第 $i$ 个子节点, 以 $D_i$ 为训练集, 以 $A-{A_g}$ 为特征集, 递归调用步1 ~ 步5, 得到子树 $T_i$, 返回 $T_i$.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;c45-算法&quot;&gt;C4.5 算法&lt;/h3&gt;

&lt;p&gt;输入: 训练数据集 $D$, 特征集 $A$, 阈值 $\epsilon$&lt;br /&gt;
输出: 决策树 $T$&lt;br /&gt;
步骤:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;若 $D$ 中所有实例输入同一类 $C_k$, 则 $T$ 为单节点树, 并将类 $C_k$ 作为该节点的类标记, 返回 $T$; (只有一个类的情况)&lt;/li&gt;
  &lt;li&gt;若 $A=\varnothing$ , 则 $T$ 为单节点树, 并将 $D$ 实例数最大的类 $C_k$ 作为该节点的类标记, 返回 $T$; (没有特征量的情况)&lt;/li&gt;
  &lt;li&gt;否则, 计算 $A$ 中各个特征对于 $D$ 的&lt;strong&gt;信息增益比&lt;/strong&gt;, 选择信息增益比最大的特征 $A_g$;&lt;/li&gt;
  &lt;li&gt;如果 $A_g$ 的信息增益比小于阈值 $\epsilon$, 则置 $T$ 为单节点树, 并将 $D$ 中实例数最大的类 $C_k$ 作为该节点的类标记, 返回 $T$;&lt;/li&gt;
  &lt;li&gt;否则, 对 $A_g$ 的每一个可能值 $a_i$, 依 $A_g = a_i$ 将 $D$ 分割为若干个非空子集 $D_i$, 将 $D_i$ 中实例数最大的类作为标记, 构建子节点, 由节点及其子节点构成树 $T$, 返回 $T$;&lt;/li&gt;
  &lt;li&gt;对于第 $i$ 个子节点, 以 $D_i$ 为训练集, 以 $A-{A_g}$ 为特征集, 递归调用步1 ~ 步5, 得到子树 $T_i$, 返回 $T_i$.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;cart-算法&quot;&gt;CART 算法&lt;/h3&gt;

&lt;p&gt;输入: 训练数据集 $D$, 基尼指数阈值, 样本个数阈值&lt;br /&gt;
输出: 决策树 $T$&lt;br /&gt;
步骤:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;对于当前节点的数据集为 $D$, 如果样本个数小于阈值或者没有特征, 返回 $T$;&lt;/li&gt;
  &lt;li&gt;计算样本集 $D$ 的基尼系数, 如果基尼系数小于阈值, 返回 $T$;&lt;/li&gt;
  &lt;li&gt;否则, 计算当前节点现有的各个特征的各个特征值对数据集 $D$ 的基尼系数, 选择基尼系数最小的特征 $A$ 和对应的特征值 $a$, 将数据集分为两个部分 $D1$ 与 $D2$, 同时建立左右两个子节点;&lt;/li&gt;
  &lt;li&gt;对于子节点递归调用步1 ~ 步3, 得到子树 $T_i$, 返回 $T_i$.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;决策树修剪&quot;&gt;决策树修剪&lt;/h2&gt;

&lt;p&gt;经过决策树生成算法得到的决策树对于训练数据具有很高的分类准确性, 但是对于验证数据集却没有那么高. 换言之就是, 产生了过拟合现象. 如同大部分算法过拟合的原因一样, 如果在使用决策树生成算法时过多得考虑正确性的话, 就容易产生过拟合. 所以需要对产生的模型进行简化, 也就是”决策树修剪 (pruning)”: 从已经生成的树上裁掉一些子树或者叶节点.&lt;/p&gt;

&lt;p&gt;具体的实现方式就是&lt;strong&gt;极小化决策树整体的损失函数&lt;/strong&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;(可以跳过)&lt;/em&gt; 再看公式, 首先定义决策树学习的损失函数为&lt;/p&gt;

\[C_\alpha(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|\notag\]

&lt;p&gt;其中 $T$ 表示树, $|T|$ 为其叶节点的个数, $t$ 是树 $T$ 的其中一个叶节点, 该叶节点上由 $N_t$ 个样本点, 其中属于 $k$ 类样本点的共有 $N_{tk}$ , 则&lt;/p&gt;

\[H_t(T)=-\sum_k\frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}\notag\]

&lt;p&gt;表示第 $t$ 个叶子的熵. 这里的 $\alpha\geq 0$ 是惩罚系数. 将 $H_t(T)$ 带入上式:&lt;/p&gt;

\[\begin{align*}
\sum_{t=1}^{|T|}N_tH_t(T)&amp;amp;=-\sum_{t=1}^{|T|}N_t\sum_k\frac{N_{tk}}{N_t}\log_2\frac{N_{tk}}{N_t}\\
&amp;amp;=-\sum_{t=1}^{|T|}\sum_{k=1}^{K}N_t\log\frac{N_{tk}}{N_t}\\
&amp;amp;=C(T)
\end{align*}\]

&lt;p&gt;这样原来的式子就成了&lt;/p&gt;

\[C_\alpha(T) = C(T)+\alpha |T|\notag\]

&lt;p&gt;$C(T)$ 的意义在于每一个叶子节点的&lt;strong&gt;熵值的加权和&lt;/strong&gt;, 如果一个节点被分到了不能再分的时候, 它的熵应该为越来越小 , 那么其熵值的加权值应该也等于越来越小. 因此, 一个树分类纯度越高, 熵值加权和 $C(T)$ 就越小. 但是于此同时, 树的复杂度变大就意味着 $|T|$ 值变大, 后一项就变大了. 整个式子 $C_\alpha(T)$ 就处在一个动态平衡的状态.&lt;/p&gt;

&lt;p&gt;极端情况下, 假设只有一个根节点的时候, 显然损失函数的值也会非常大. 另一种, 当树非常复杂的时候, 前一项基本为 0, 但后一项又变得非常大, 整体的损失函数也会很大. 这中间存在一个极小值的点, 使得损失函数最小. 而决策树修剪就是通过动态删减一些叶子节点, 找到这个极小值.&lt;/p&gt;

&lt;p&gt;总结树的修剪算法.&lt;/p&gt;

&lt;p&gt;输入: 生成算法产生的整个树 $T$ , 惩罚参数 $\alpha$;&lt;br /&gt;
输出: 修剪后的子树 $T_\alpha$&lt;br /&gt;
步骤:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;计算每个节点的经验熵&lt;/li&gt;
  &lt;li&gt;利用损失函数, 递归地从树的叶节点向上回缩, 找到极小值&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在实际算法使用过程中, 会留出一部分数据集作为验证数据集, 用于学习过程中的泛化性评价, 以防止出现过拟合.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;这样就完成了决策树的建立. 正如一开始所说, 决策树的优点在于, 易于理解和解释, 能够使用工具进行可视化分析. 另外数据集中出现缺失值也不影响树的建立. 其缺点在于容易产生过拟合. 三个算法各自也有其缺点, 比如ID3算法计算信息增益时结果偏向数值比较多的特征等等, 这里暂时不展开.&lt;/p&gt;

&lt;p&gt;以上就是对决策树系列算法的一个简单概括, 其中很多细节的地方并不是很周到. 后续会对于 ID3, C4.5 和 CART 三个算法逐个详细解释以及基于 Python 实现.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;参考文章:&lt;/p&gt;

  &lt;ol&gt;
    &lt;li&gt;李航《统计学习方法》&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/jiaoyangwm/article/details/79525237&quot;&gt;机器学习实战（三）——决策树&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://easyai.tech/ai-definition/decision-tree/&quot;&gt;决策树 – Decision tree&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.cnblogs.com/pinard/p/6053344.html&quot;&gt;决策树算法原理(下)&lt;/a&gt;&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;这篇文章只考虑分类的情况, 决策树回归的内容会另外整理一篇. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Mon, 02 Nov 2020 14:50:09 +0900</pubDate>
        <link>https://simcookies.github.io/2020/11/02/summary-of-machine-learning-algorithms-decision-tree</link>
        <guid isPermaLink="true">https://simcookies.github.io/2020/11/02/summary-of-machine-learning-algorithms-decision-tree</guid>
        
        <category>algorithm</category>
        
        <category>formula</category>
        
        
        <category>machine learning</category>
        
      </item>
    
      <item>
        <title>树莓派上安装并使用 Mosquitto</title>
        <description>&lt;p&gt;在物联网设备开发中经常需要用到 MQTT 协议进行通信. MQTT 是一个基于发布 (Publish) 和 订阅 (Subscribe) 的消息协议. 位于通信中心的设备称为 MQTT broker. 其他的设备之间的消息传递都是经由这个 broker. 以前一直选择 &lt;a href=&quot;https://www.cloudmqtt.com/&quot;&gt;CloudMQTT&lt;/a&gt;, Free Trail是足够一般情况下的使用, 但是速度上存在一定的问题 (毕竟服务器在美国). 另外, 在自定义特性上面也没有那么的好.&lt;/p&gt;

&lt;p&gt;相比之下, Mosquitto 是一款开源的 MQTT broker 软件. 包括了服务端和客户端. 在实际的开发中, 经常将 broker 设置在本地. 这篇文章就记录了如何在树莓派上安装并且使用 Mosquitto.&lt;/p&gt;

&lt;h1 id=&quot;mosquitto-的安装和配置&quot;&gt;Mosquitto 的安装和配置&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;这里使用的树莓派中, 安装的是 Raspbian. 因为是基于 Debian 的, 使用方法基本上没有差别.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;安装&quot;&gt;安装&lt;/h2&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;mosquitto
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这样就可以了. mosquitto 作为一个服务一般都是后台运行的 (作为一个daemon), 安装成功之后就会自动开始运行了. 使用 service 查看运行状况.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;service mosquitto status
● mosquitto.service - Mosquitto MQTT v3.1/v3.1.1 Broker
   Loaded: loaded &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;/lib/systemd/system/mosquitto.service&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; enabled&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; vendor preset: enabled&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
   Active: active &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;running&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; since Sun 2020-09-27 15:50:35 JST&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; 34min ago
     Docs: man:mosquitto.conf&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;5&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
           man:mosquitto&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;8&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
 Main PID: 14971 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;mosquitto&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    Tasks: 1 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;limit: 2319&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
   Memory: 788.0K
   CGroup: /system.slice/mosquitto.service
           └─14971 /usr/sbin/mosquitto &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; /etc/mosquitto/mosquitto.conf

Sep 27 15:50:35 openhab systemd[1]: Starting Mosquitto MQTT v3.1/v3.1.1 Broker...
Sep 27 15:50:35 openhab systemd[1]: Started Mosquitto MQTT v3.1/v3.1.1 Broker.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;从上面的输出的 Active 可以得到 mosquitto 运行正常. 另外通过 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;netstat&lt;/code&gt; 指令查看这个服务的接口 IP 以及 Port 的状态.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;netstat &lt;span class=&quot;nt&quot;&gt;-ltnp&lt;/span&gt;
Active Internet connections &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;only servers&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:1883            0.0.0.0:&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;               LISTEN      14971/mosquitto
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;可以看出服务面向了所有 IP 地址, 以及使用了 1883 端口, 而这些都是 mosquitto 的默认配置.&lt;/p&gt;

&lt;h2 id=&quot;配置&quot;&gt;配置&lt;/h2&gt;

&lt;p&gt;首先为了保证最基本的安全通信, &lt;strong&gt;可以为 broker 设置用户名和密码.&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;mosquitto_passwd &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; /etc/mosquitto/passwd username
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;为一个叫做 username 的人生成一个相应的密码, 存储在 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/mosquitto/passwd&lt;/code&gt; 中. 并且在配置文件  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/mosquitto/mosquitto.conf&lt;/code&gt; 中进行指定.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;allow_anonymous &lt;span class=&quot;nb&quot;&gt;false
&lt;/span&gt;password_file /etc/mosquitto/passwd
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这样在之后的连接中, 就能够使用用户名 username 和密码进行登录来了. 如果并不需要用户名密码登录的话, 可以设置匿名用户登录, 这样任何设备都可以直接连接 broker 了.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;allow_anonymous &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;另外通过修改配置文件也可以修改开放 IP 和端口&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bind_address 0.0.0.0 &lt;span class=&quot;c&quot;&gt;# 使用 ip 地址或者主机名&lt;/span&gt;
port 1883            &lt;span class=&quot;c&quot;&gt;#开发端口&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;有的时候在一些特殊情况下不支持基本的 MQTT 协议, 比如 Python 或者 JavaScript 的 Paho 包, 需要使用 websockets 的协议. 同样也是在配置文件中修改.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;listener 8083
protocol websockets
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;不同于基本的 MQTT broker, 这里相当于新建一个 websockets 协议, 8083 端口的新的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;listener&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;以上基本的配置就结束了. 需要注意的是, 文件末尾必须留下换行符号, 而且配置好文件之后需要重新启动 mosquitto 服务.&lt;/p&gt;

&lt;h1 id=&quot;使用-mqttbox-进行测试&quot;&gt;使用 MQTTBox 进行测试&lt;/h1&gt;

&lt;p&gt;为了测试上述的安装与配置, 以及实际的通信没有问题, 可以使用一个 &lt;a href=&quot;http://workswithweb.com/mqttbox.html&quot;&gt;MQTTBox&lt;/a&gt; 的小工具. 当然也是可以使用 mosquitto 自带的 clients 功能, 只不过没有那么可视化.&lt;/p&gt;

&lt;p&gt;最开始添加一个 MQTT Client. 填入红框中的基本参数就可以了.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20200927172533.png&quot; alt=&quot;Snipaste_2020-09-27_17-24-49&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其中 “MQTT Client Name” 随意记入. 因为上述的配置中没有选择特殊的 websockets, 所以 Protocol 中选择 mqtt/tcp, 而不选择 ws. Host 中填入安装 mosquitto 的设备的可访问的局域网 IP 地址. 最后填入用户名和密码保存就可以了.&lt;/p&gt;

&lt;p&gt;显示连接成功的话, 就可以进行测试. 一开始 MQTTBox 会默认打开一个 publish 和一个 subscribe. 通过发布和订阅一样的 Topic (例中使用了 EdgeDevice/Status 作为 Topic) 就能测试通信了.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20200927173300.png&quot; alt=&quot;Snipaste_2020-09-27_17-32-04&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;左侧发布的 Payload 的内容, 右侧订阅也能收到一样的内容就表示通信没有问题.&lt;/p&gt;

&lt;h1 id=&quot;特殊的-topic&quot;&gt;特殊的 Topic&lt;/h1&gt;

&lt;p&gt;在 mosquitto 中有一些特殊的主题, 通过订阅他们可以获得当前 broker 的实时情况. 这些内容可以使用 man 手册查看 mosquitto 得到, 这里我列出两个一下个人觉得有用的.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Topic&lt;/th&gt;
      &lt;th&gt;内容&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$SYS/broker/clients/connected&lt;/td&gt;
      &lt;td&gt;当前连接着的客户端数量&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$SYS/broker/clients/expired&lt;/td&gt;
      &lt;td&gt;因为超过持续连接时间而中断的客户端数量&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;trouble-shouting&quot;&gt;Trouble shouting&lt;/h1&gt;

&lt;p&gt;基本配置能够保证绝大部分的通信, 但比如下面 Log 中的错误.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Client XXX has exceeded timeout, disconnecting.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这只是因为在 keepalive 时间内(默认15秒) 没有进行任何通信而产生的自动断开, 并不是Mosquitto的问题, 而是通信设备连接软件的问题.&lt;/p&gt;

&lt;p&gt;解决办法也很简单:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;在 keepalive 时间内保证通信;&lt;/li&gt;
  &lt;li&gt;在主循环中使用client.loop()方法保证通信.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 27 Sep 2020 15:53:04 +0900</pubDate>
        <link>https://simcookies.github.io/2020/09/27/mosquitto-mqtt-on-raspberry-pi</link>
        <guid isPermaLink="true">https://simcookies.github.io/2020/09/27/mosquitto-mqtt-on-raspberry-pi</guid>
        
        <category>mqtt</category>
        
        <category>raspberrypi</category>
        
        <category>server</category>
        
        
        <category>tool</category>
        
      </item>
    
      <item>
        <title>使用 PicGo 与 Github 为 Typora 搭建图床</title>
        <description>&lt;p&gt;一直使用 Markdown 编辑器 &lt;a href=&quot;https://typora.io/&quot;&gt;Typora&lt;/a&gt;, 因为除了最基本的 Markdown 语法之外, 他还非常好地支持了 LaTex 公式以及 Mermaid 图表库. 但是之前一直有些在意的地方就是 Typora 的图片存放位置的问题. 之前的版本支持复制到当前文件夹或者其他指定文件夹, 但是这样就导致了, 每次同步或者拷贝某一个 md 文档的时候, 要带着这个图片文件夹一起”跑”, 感觉总是怪怪的.&lt;/p&gt;

&lt;p&gt;从版本 0.9.84 开始忽然收到更新通知, 开始支持图片上传功能了(通过 PicGo 或者其他自定义命令)! 在这里我就用了官方推荐的 &lt;strong&gt;PicGo 加上 Github 仓库的方案&lt;/strong&gt;尝试了一段时间, 感觉确实不错. 图片可以存放在一个地方, 而且每次移动文档时也不用担心其中的图片丢失链接了. 这篇文章里, 我想大概总结一下这个方案的做法.&lt;/p&gt;

&lt;h1 id=&quot;github-设置&quot;&gt;Github 设置&lt;/h1&gt;

&lt;p&gt;可以新建或者用现有的仓库&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, 这个仓库就是用来存放未来所有上传上来的图片的, 当作云存储来用的感觉. 这里的仓库名称和 branch 名称留以备用.&lt;/p&gt;

&lt;p&gt;另外为了让 PicGo 能够上传图片至 Github, 需要给予其访问权限, 这里 PicGo 采用的就是 token. 在 Github 设置的地方新建一个 token 备用.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20200628000404.png&quot; alt=&quot;image-20200628000403915&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;下载安装-picgo&quot;&gt;下载安装 &lt;a href=&quot;https://molunerfinn.com/PicGo/&quot;&gt;PicGo&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;安装好之后的 PicGo 长成这个样子&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20200627235009.png&quot; alt=&quot;image-20200627235009023&quot; /&gt;&lt;/p&gt;

&lt;p&gt;开始对 Github 图床进行设置, 把上面所提到的仓库名, 分支名, Token 都填上 (有需要的话, 仓库中的存储目录也是可以指定的), 基本上就算是结束了.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20200628001038.png&quot; alt=&quot;image-20200628001038711&quot; /&gt;&lt;/p&gt;

&lt;p&gt;个人习惯性会将图片的名称全部自动改为日期之后上传, 正好 PicGo 自带了这样的设置 (其他设置可自由折腾):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20200628001409.png&quot; alt=&quot;image-20200628001409831&quot; /&gt;&lt;/p&gt;

&lt;p&gt;到了这里, PicGo 自身已经可以用来上传图片了. 在上传区中”点击上传”或者”拖拽文件上传”都可以实现上传图片到 Github. 而 Github 仓库里面也能够看到传过来的图片了. 最后开始设置 Typora.&lt;/p&gt;

&lt;h1 id=&quot;typora-连接-picgo&quot;&gt;Typora 连接 PicGo&lt;/h1&gt;

&lt;p&gt;在偏好设置 &amp;gt; 图像里面选择: 插入图片时上传图片, 并且设置上传服务为 PicGo(App) 并且指定一下 PicGo 的安装路径. 最后可以使用&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;验证图片上传选项&lt;/code&gt; 来查看效果, 会返回上传成功的 JSON 消息.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20200628001949.png&quot; alt=&quot;image-20200628001949008&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20200628002355.png&quot; alt=&quot;image-20200628002355088&quot; /&gt;&lt;/p&gt;

&lt;p&gt;同时, 在 Github 的图床里面会多出来两张 Typora 的 logo. 至此, 所有的设置都完成了, 再次向 Typora 中插入或者直接拷贝图片的时候, 便会自动触发 PicGo 自动上传图片到 Github 功能. 加上 OneDrive, 一个本地的 Markdown 编辑器 Typora 彻底变成了一个方便易用的云端笔记工具 (真的推荐 Latex 公式功能) :smiley:&lt;/p&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;这个代码仓库必须要是 public 性质的才能在各个地方都能够被访问到. 这里涉及到了图片访问权限的问题, 所以请针对情况使用. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;其中图床设置的地方应当有很多项, 但是因为其他项暂时都用不到, 所以都关闭了 &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sat, 27 Jun 2020 23:21:01 +0900</pubDate>
        <link>https://simcookies.github.io/2020/06/27/make-image-host-service-for-typora-with-picgo-and-gihub</link>
        <guid isPermaLink="true">https://simcookies.github.io/2020/06/27/make-image-host-service-for-typora-with-picgo-and-gihub</guid>
        
        <category>typora</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Summary of Machine Learning Algorithms -- Support Vector Machine</title>
        <description>&lt;p&gt;&lt;strong&gt;Support Vector Machine&lt;/strong&gt; (SVM) has become a more and more popular algorithm in the field of ML, even some times more than &lt;strong&gt;Neural Network&lt;/strong&gt; (NN). It is widely used for classification and regression. So, in this post I will give a brief about the SVM.&lt;/p&gt;

&lt;h1 id=&quot;brief-introduction&quot;&gt;Brief Introduction&lt;/h1&gt;

&lt;p&gt;SVM is a kind of supervised learning method, it has good performance for classification problems. And it is a non-probabilistic way of learning, which means it can can directly get the result of the classification instead of the probability of the class which it belongs to (like logistic regression). In addition, the support of the “&lt;strong&gt;Nuclear Function&lt;/strong&gt;” in SVM also makes it have a wider range of use. The concept of “nuclear function” is also used in other classification learning methods.&lt;/p&gt;

&lt;h1 id=&quot;support-vector-machine&quot;&gt;Support Vector Machine&lt;/h1&gt;

&lt;p&gt;First, let’s say we have two types of data, one positive type and one negative type. Features have two dimensional data: $\vec{x} = (x_1, x_2)$, which can be plotted like below (Orange for positive data and blue for negative data):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20201102144404.png&quot; alt=&quot;SVM original dataset&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now we want to draw a line to separate the two types of data. There are three lines to choose from below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20201102144443.png&quot; alt=&quot;SVM with different classfication lines&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Obviously, the middle (b) is the best way to separate the data. So how can we count the “most able to” separate the data? We can assume a hyperplane which divide dataset into two classes. And this plane has as longer distance as possiable to all the two kinds of data. We call it **separating hyperplane **(or decision boundray). Equation for hyperplane (here is the line (b) in the figure):
\(\vec w\cdot \vec x + b=0\)
$\vec w$ is the vector perpendicular to the hyperplane. The magnitude of $\vec w$ is unkown. $\vec x$ are the vectors on the hyperplane. $b$ is bias.&lt;/p&gt;

&lt;p&gt;For negative objects:
\(\vec w\cdot \vec x_- + b\leq1\)
For positive objects:
\(\vec w\cdot \vec x_+ + b\geq1\)
The thing what we want to do is &lt;strong&gt;very easy&lt;/strong&gt;,  it just is checking the result of:
\(\text{sgn}(\vec w\cdot\vec x+b)\)
If it’s plus then it belongs to positive class, otherwise it belongs to negitive class.&lt;/p&gt;

&lt;p&gt;The big problem is how to find the unknow $\vec w$ and $b$. We can select two parallel hyperplanes that separate the two classes of data, so that the distance between them is as large as possible. The distance between these hyperplanes is called &lt;strong&gt;margin&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The two hyperplanes can be described by:&lt;/p&gt;

\[\vec w\cdot \vec x_i + b=1 \\
\vec w\cdot \vec x_i + b=-1\]

&lt;p&gt;Here is another example (Red for positive and blue for negative).  The two gray lines are the margin.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20201102144506.png&quot; alt=&quot;svm_margin&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If $y_i$ presents the class of $i$-th piece of data, 1 for positive and -1 for negitive, then the two equations can be written into one:
\(y_i(\vec w \cdot \vec x_i + b) =1\)&lt;/p&gt;

&lt;p&gt;Those vectors in the plane are called &lt;strong&gt;support vector&lt;/strong&gt;. The width between two hyperplanes:&lt;/p&gt;

\[\begin{align}
\text{Width} &amp;amp;= (\vec x_+-\vec x_-)\cdot \frac{\vec w}{||\vec w||} \notag\\
&amp;amp;=(\vec w\cdot\vec x_+-\vec w\cdot\vec x_-)\cdot \frac{1}{||\vec w||} \notag\\
&amp;amp;=(1-b+1+b)\cdot \frac{1}{||\vec w||} = \frac{2}{||\vec w||}
\end{align}\]

&lt;p&gt;The thing what we want to do is maxmum the width:&lt;/p&gt;

\[\text{max Width}\to\text{max}\frac{2}{||\vec w||} \to \text{min}||\vec w||\to\text{min}\frac{1}{2}||\vec w||^2\]

&lt;p&gt;Now the problem becomes:&lt;/p&gt;

\[\text{min} \frac{1}{2}||\vec w||^2, s.t., y_i(\vec w \cdot \vec x_i + b) -1\geq0, i = 1,2\dots,n\]

&lt;p&gt;This problem can be solved by &lt;strong&gt;Lagrange multiplier&lt;/strong&gt;:&lt;/p&gt;

\[L(\vec w, b, \alpha) = \frac{1}{2}||\vec w||^2-\sum_{i=1}^{n}\alpha_i\left[y_i(\vec w\cdot\vec x_i + b)-1\right]\]

&lt;p&gt;Now we need know partial difference $L$ with respect to $\vec w$ and $b$ and let them equal to 0:&lt;/p&gt;

\[\begin{align}
\frac{\partial{L}}{\partial \vec w_i} &amp;amp;= \vec w - \sum_{i=1}^{n}\alpha_i y_i \vec x_i = 0\notag\\
\frac{\partial{L}}{\partial b} &amp;amp;= - \sum_{i=1}^{n}\alpha_i y_i  = 0
\end{align}\]

&lt;p&gt;We got the $\vec w$, and bring it back to the Largrange muptiplier:&lt;/p&gt;

\[\begin{align}
L(\vec w, b, \alpha) &amp;amp;= \frac{1}{2}\left(\sum_{i=1}^n\alpha_iy_i\vec x_i\right)^2-\sum_{i=1}^n\alpha_iy_i\vec x_i\left(\sum_{i=1}^n\alpha_iy_i\vec x_i\right)-\sum_{i=1}^n\alpha_iy_ib+\sum_{i=1}^n\alpha_i\notag\\
&amp;amp;=\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^n\alpha_i\alpha_jy_iy_j\vec x_i\vec x_j-\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j\vec x_i \vec x_j - b\sum_{i=1}^n\alpha_iy_i+\sum_{i=1}^n\alpha_i\notag\\
&amp;amp;=\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^n\alpha_i\alpha_jy_iy_j\vec x_i\vec x_j
\end{align}\]

&lt;p&gt;This is a quadratic problem. The most popular way to solve the problem is sequential minimal optimization, &lt;strong&gt;SMO&lt;/strong&gt;(Microsoft).&lt;/p&gt;
</description>
        <pubDate>Sun, 02 Dec 2018 10:10:42 +0900</pubDate>
        <link>https://simcookies.github.io/2018/12/02/support-vector-machine</link>
        <guid isPermaLink="true">https://simcookies.github.io/2018/12/02/support-vector-machine</guid>
        
        <category>algorithm</category>
        
        <category>formula</category>
        
        
        <category>machine learning</category>
        
      </item>
    
      <item>
        <title>Summary of Machine Learning Algorithms -- Linear Regression</title>
        <description>&lt;p&gt;&lt;strong&gt;Regression&lt;/strong&gt; is one class of problems in Machine Learning. Linear Regression is a basic one class of problems of Regression. I want to write a note about the Linear Regression in this post as the beginning of Machine Learning series.&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;brief-introduction&quot;&gt;Brief Introduction&lt;/h1&gt;

&lt;p&gt;Regression problems want to find the relationship between the input variables and output variables. The &lt;em&gt;Regression&lt;/em&gt; was used from a 19th-Century scientist.
&lt;em&gt;Linear Regression&lt;/em&gt; is most basic problems of Regression. We want to make a model to describe the relationship between input and output.
So let’s assume the input variables are $x_1, x_2, \dots , x_n$, and the output variable is $y$. This formula shows the linear relationship between them:
\(y=m_1x_1+m_2x_2 + \dots+m_nx_n+b=m^Tx+b\)&lt;/p&gt;

&lt;p&gt;Here, $m_i$ is coefficient. The $m = (m_1, m_2, \dots, m_n)$ and $x = (x_1, x_2, \dots, x_n)$ are vectors, the $b$ calls bias. We have the model now, what we want to do next is to find the $m$ and $b$ with given &lt;strong&gt;dataset&lt;/strong&gt; and then, we can use the model to make some prediction of some unknown data.&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;the-case-of-2-dimension&quot;&gt;The case of 2-dimension&lt;/h1&gt;

&lt;p&gt;For the 2-dimension (one input variable $x$ with one output variable $y$), we can get $m$ and $b$ easily. The model formula becomes $y=mx+b$ which is a very simple line and we call it the &lt;em&gt;best fit line&lt;/em&gt;. \(m\) is the best fit line slope and $b$ is best fit line y-intercept.&lt;/p&gt;

&lt;p&gt;Now, we have one dataset with $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \dots, (x^{(m)}, y^{(m)})$. Here is the plot of them:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20201102144249.png&quot; alt=&quot;dataset of linear regression&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can get $m$ and $b$ quite easily with these formulas (from &lt;strong&gt;statistics&lt;/strong&gt;):&lt;/p&gt;

\[\begin{align}
m&amp;amp;=\frac{\overline x \overline y - \overline{xy}}{\overline x^2 - \overline {x^2}} \\
b&amp;amp;=\overline y - m\overline x
\end{align}\]

&lt;p&gt;After this, we can draw the best fit line as blue line, which is also our model. According this line, we can make prediction of $x=40$ which plot as red point:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20201102144312.png&quot; alt=&quot;linear regression prediction&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;for-the-generalized-case&quot;&gt;For the generalized case&lt;/h1&gt;

&lt;p&gt;For the generalized case, $m$ is a vector. So we cannot get it easily just with those formulas upper. Now assume the $h(x)$ is the &lt;strong&gt;Hypothesis&lt;/strong&gt; model:&lt;/p&gt;

\[h(x^{(i)})=m^Tx^{(i)}+b\]

&lt;p&gt;We can use a &lt;strong&gt;cost function&lt;/strong&gt; – &lt;strong&gt;Mean Squared Error&lt;/strong&gt; (MSE) to evaluate this model:&lt;/p&gt;

\[J(m, b) = \frac{1}{2n}\sum_{i=1}^n\left[h(x^{(i)})-y^{(i)}\right]^2\]

&lt;p&gt;$1/2n$ here is just for calculating convenience. What we need to do it to find the $m$ and $b$ to minimize the MSE which also means the line we made is as close as possible to those dataset points:&lt;/p&gt;

\[\min_{m, b}J(m,b)\]

&lt;p&gt;Now the &lt;strong&gt;Find unknown $m$ and $b$&lt;/strong&gt; becomes &lt;strong&gt;Find the min value of $J$, and get $m$ and $b$&lt;/strong&gt;. This problem can be solved by &lt;strong&gt;Gradient Descent&lt;/strong&gt; (which I want to give a another post to give some details).&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;evaluation-metrics&quot;&gt;Evaluation Metrics&lt;/h1&gt;

&lt;p&gt;Finally, after we get the value of $m$ and $b$, we will get the hypothesis model. Usually, we use &lt;strong&gt;Coefficient of determination&lt;/strong&gt; (witch also calls $\text{R}^2$) to evaluate this model.&lt;/p&gt;

&lt;p&gt;Firstly, define the &lt;strong&gt;residuals&lt;/strong&gt; as $e^{(i)} = y^{(i)} - h(x^{(i)})$, and &lt;strong&gt;mean&lt;/strong&gt; of the data as $\overline{y}$, then define these sums of squares:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The total sum of squares (proportional to the variance of the data):&lt;/p&gt;

\[SS_{\text{tot}} = \sum_i(y^{(i)} - \overline y)^2\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The regression sum of squares, also called the explained sum of squares:&lt;/p&gt;

\[SS_{\text{reg}} = \sum_i(h(x^{(i)}) - \overline y)^2\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The sum of squares of residuals, also called the residual sum of squares:&lt;/p&gt;

\[SS_{\text{res}} = \sum_i(y^{(i)} - h(x^{(i)}))^2 = \sum_i(e^{(i)})^2\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;lastly, define the coefficient of determination.&lt;/p&gt;

\[\text{R}^2 = \frac{SS_{\text{reg}}}{SS_{\text{tot}}} = 1- \frac{SS_{\text{res}}}{SS_{\text{tot}}}\]

&lt;blockquote&gt;
  &lt;p&gt;The better the linear regression fits the data in comparison to the simple average, the closer the value of $\text{R}^2$ is to 1.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;practice-corner&quot;&gt;Practice corner&lt;/h1&gt;

&lt;p&gt;For the 2-D case, it’s quite easy to achieve that:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;statistics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;best_fit_slope_and_intercept&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# train dataset
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;best_fit_slope_and_intercept&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;regression_line&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# prediction
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;predict_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predict_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# data visualization
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;regression_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predict_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For generalized case, we can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sklearn&lt;/code&gt; package. here is the code sketch:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearRegression&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Prepare some train dataset and test dataset
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearRegression&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;clf.score&lt;/code&gt; will return the value of coefficient of determination $\text{R}^2$.&lt;/p&gt;

&lt;p&gt;In fact, the data preprocess is quite important step, it includes data cleaning and feature selection.
That’s a big topic, so I want to make another post to give details.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_regression&quot;&gt;Linear Regression (Wiki)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://pythonprogramming.net/regression-introduction-machine-learning-tutorial/&quot;&gt;pythonprogramming.net&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Fri, 21 Sep 2018 14:53:00 +0900</pubDate>
        <link>https://simcookies.github.io/2018/09/21/summary-of-machine-learning-linear-regression</link>
        <guid isPermaLink="true">https://simcookies.github.io/2018/09/21/summary-of-machine-learning-linear-regression</guid>
        
        <category>algorithm</category>
        
        <category>formula</category>
        
        
        <category>machine learning</category>
        
      </item>
    
  </channel>
</rss>
