<!DOCTYPE html><html><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>机器学习算法系列 - 最小二乘法/岭回归/Lasso/Elastic-Net</title><meta name="description" content="普通最小二乘法(Ordinary Least Squares, OLS), 岭回归(Ridge Regression), 最小绝对值收留和选择算子(Least absolute shrinkage and selection operator, Lasso), 弹性网络(Elastic-Net), 这四个算法是线..."><link rel="shortcut icon" type="image/x-icon" href="/public/favicon.ico"><link rel="stylesheet" href="/public/css/main.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.5.0/css/font-awesome.css" integrity="sha512-EaaldggZt4DPKMYBa143vxXQqLq5LE29DG/0OoVenoyxDrAScYrcYcHIuxYO9YNTIQMgD8c8gIUU8FQw7WpXSQ==" crossorigin="anonymous"><link rel="canonical" href="https://simcookies.github.io/2021/07/17/summary-of-machine-learning-algorithms-linear-model-1"><link rel="alternate" type="application/rss+xml" title="Simcookies" href="https://simcookies.github.io/feed.xml"><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js" integrity="sha512-uto9mlQzrs59VwILcLiRYeLKPPbS/bT71da/OEBYEwcdNUk8jYIy+D176RYoop1Da+f9mvkYrmj5MCLZWEtQuA==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/simple-jekyll-search/1.7.12/simple-jekyll-search.min.js" integrity="sha512-APy7Ff/y4pdIHleqiTMcRZ8Wu6tjfqhJpgAcaCvTuFQPj/G+/A5u0uZi/DkfkuLQ6FeFmDJgh/zl0y3If/VUyA==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.2/mermaid.min.js" integrity="sha512-x8NWYlEsC86ngfO24tbxW6pMuyn9gYnwEW0FcSobohDc3iLCXhmRkqYXgTfE7Jwy2YCTnHRfyum8LUIiyvmgWQ==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha512-Fv0JVzxvrWFLhiUP8wZYR6VwBZQt0XqB9MhHZE+12lObqIDzg3LOJcCCiRy6g4MCX/MlG+R9IRCWZAHjo9iBgw==" crossorigin="anonymous"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      showProcessingMessages: false,
      messageStyle: "none",
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
      }
    });</script><script src="/public/script/main.js"></script></head><body><header class="site-header"><div class="wrapper"><a class="site-title" href="/">Simcookies</a> <span class="menu-icon" id="menu-icon"><span class="fa-stack fa-lg"><i class="fa fa-square-o fa-stack-2x"></i> <i class="fa fa-bars fa-stack-1x"></i></span></span><nav class="site-nav"><div class="trigger"><a class="page-link" href="/about/">About</a> <a class="page-link" href="/archive/">Archive</a> <a class="page-link" href="/wiki/">Wiki</a> <a class="page-link" href="/feed.xml">Feeds</a> <a class="page-link" href="https://github.com/simcookies/gitio" target="_blank" rel="noopener noreferrer"><i class="fa fa-github-alt fa-lg"></i> Repositories</a></div></nav></div></header><div class="scroll-content"><div class="page-content"><div class="wrapper"><article class="post" itemscope itemtype="http://schema.org/BlogPosting"><header class="post-header"><h1 class="post-title" itemprop="name headline">机器学习算法系列 - 最小二乘法/岭回归/Lasso/Elastic-Net</h1><p class="post-meta"><time datetime="2021-07-17T16:36:31+09:00" itemprop="datePublished">Jul 17, 2021</time>  <i class="fa fa-archive" aria-hidden="true"></i>  <span><a class="category-div" href="/category/machine-learning">machine learning</a></span>  <i class="fa fa-tag"></i>  <a class="tag-div" href="/tag/algorithm">algorithm</a>  <a class="tag-div" href="/tag/formula">formula</a> </p></header><div class="post-content" itemprop="articleBody"><p><strong>普通最小二乘法</strong>(Ordinary Least Squares, OLS), <strong>岭回归</strong>(Ridge Regression), 最小绝对值收留和选择算子(Least absolute shrinkage and selection operator, <strong>Lasso</strong>), 弹性网络(<strong>Elastic-Net</strong>), 这四个算法是线性模型中比较基本的算法. 这一篇文章里一次性对这四个进行一个简单的总结.</p><p>首先, 因为四者都是线性模型, 所以首先假设我们该模型为:</p>\[y(w,x)=w_0+w_1x_1+\dots+w_mx_m\notag\]<p>其中 $w=(w_1,\dots,w_m)$ 称为系数 (coefficient), 而 $w_0$ 称为截距 (intercept), 而四个算法就是为了找出系数和截距. 使用矩阵的方式表达显得更亲切:</p>\[y=\begin{bmatrix}y_1\\y_2\\\vdots\\y_n\end{bmatrix}, X=\begin{bmatrix} 1&amp;x_{11}&amp;x_{12}&amp;\dots&amp;x_{1m}\\ 1&amp;x_{21}&amp;x_{22}&amp;\dots&amp;x_{2m}\\ \vdots&amp;\vdots&amp;\vdots&amp;\dots&amp;\vdots\\ 1&amp;x_{n1}&amp;x_{n2}&amp;\dots&amp;x_{nm} \end{bmatrix},w=\begin{bmatrix}w_0\\w_1\\w_2\\\vdots\\w_m\end{bmatrix}\\\notag y=Xw\]<h1 id="普通最小二乘法">普通最小二乘法</h1><p>最小二乘法可能是我接触到最早的机器学习的算法了. 研究生期间经常通过推导出的传递函数去拟合电路仿真的理想结果, 从而确定电路参数, 这本身就是最小二乘法的一个非常重要的应用 – <a href="https://zh.wikipedia.org/wiki/%E6%9B%B2%E7%B7%9A%E6%93%AC%E5%90%88" target="_blank" rel="noopener noreferrer">曲线拟合</a>. 而最小二乘法中的线性或者说普通的最小二乘法, 作为统计回归分析中的一个手段, 是比较容易理解的一个算法. 其可以分解成:</p><ol><li><p>随机给出一组系数 $w$;</p></li><li><p>使用 $w$ 结合数据集的特征量 $X$ 计算出预测值 $Xw$;</p></li><li><p>计算预测值和实际值之间的差, 即模型的损失函数, 这里特指残差平方和 (也就是L2范数) $e^2=\left|y-Xw\right|_2^2$</p></li><li><p>求出使得残差平方和取得最小值的 $w$<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup>, 也可以使用更简单的矩阵的方法求出 $w$.</p></li></ol><p>用一个简单的图表示出来就是:</p><p><img src="https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20210619195655.png" alt="OLS"></p><p>一堆蓝色的点为观测数据, 看着是满足线性关系的感觉. 而橙色的直线就近似的表达了 $x$ 和 $y$ 之间这种线性的关系, 蓝色短线就是预测值和实际数据之间残差大小. 使得所有蓝线距离之和最小的参数就是我们要找的参数. 具体的数学表达为:</p>\[\min_{w}\left\|y-Xw\right\|_2^2=\min_w\sum_{i=1}^n\left(y_i-x_i^Tw\right)^2\notag\\ \rightarrow \hat w=(X^TX)^{-1}X^Ty\]<p>这里的 $\hat w$ 是利用矩阵的方式求得的参数结果.</p><hr><p><strong>矩阵的解法(TL;DR)</strong>:</p><p>假设满足使得 $X$ 和 $y$ 线性关系的参数矩阵为 $\hat w$. 那么就有:</p>\[\begin{align*} X\hat w &amp;= y\\ X^TX\hat w &amp;= X^Ty\\ \hat w&amp;=(X^TX)^{-1}X^Ty \end{align*}\]<p>而最后一步成立的条件就是 $X^TX$ 是可逆的, 也就是说是满秩的. 但是如果特征量之间的相关性很强时 (也称之为变量的共线性), $X^TX$ 就会很小, 甚至趋于0. 这个时候就需要对这个矩阵做些处理了, 也就带来了岭回归.</p><hr><p>在 Scikit-Learn 中, 实现普通最小二乘法算法的是 <code class="language-plaintext highlighter-rouge">LinearRegression</code> 模型.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>

<span class="c1"># 生成数据
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span>
    <span class="n">n_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mi">4</span>
<span class="p">)</span>

<span class="c1"># 训练模型
</span><span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># 配置参数并绘图
</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">model</span><span class="p">.</span><span class="n">intercept_</span>
<span class="n">y_dash</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_dash</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">"C1"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div><p>模型的参数反映在 <code class="language-plaintext highlighter-rouge">coef_</code> 和 <code class="language-plaintext highlighter-rouge">intercept_</code> 中, 有了这两个值就得到了近似直线. 绘图结果:</p><p><img src="https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20210619203040.png" alt="OLS_sklearn"></p><p>最小二乘法估算系数的手段在统计学中也称为<strong>最小二乘估计</strong>. 正如上面提到的为了能够使用最小二乘估计得到唯一的参数解, 特征量 $X$ 必须满足线性无关. 如果线性相关了, 得到的解会对特征量非常的敏感, 这种情况称为<strong>多重共线性</strong> (multicollinearity), 因此数据一定要经过处理, <strong>特别是要去除掉高度相关的特征量</strong>, 也就出现了改良的算法.</p><h1 id="岭回归">岭回归</h1><p>有时也被称作脊回归、吉洪诺夫正则化(Tikhonov regularization). 可以看作是最小二乘法的改良版. 其目的在于改善普通最小二乘法的多重共线性问题. 主要做法是向损失函数增加一个L2范数的惩罚项:</p>\[\begin{align*} &amp;\min_w\left\|Xw-y\right\|_2^2+\lambda\left\|w\right\|_2^2\\ =&amp;\min_w\sum_{i=1}^m\left(y_i-x_i^Tw\right)^2+\lambda\sum_{i=1}^nw_i^2 \end{align*}\]<p>其中 $\lambda\geq0$ (有时也用 $\alpha$, 两者是等价的) 称为正则化参数, 复杂系数或者岭参数, 该值越大对于异常的 $w$ 的惩罚越大, 使得训练得到的系数对于多重共线性问题更加的稳健. 以矩阵的形式表达出来:</p>\[\hat w =(X^TX+\lambda I)^{-1}X^Ty\notag\]<p>L2范数惩罚项的加入, 使得原本的 $(X^TX)$ 变成了 $(X^TX+\lambda I)$ 从而保证了满秩和可逆. 但同时也放弃了最小二乘法的无偏性, 以降低精度为代价解决了病态矩阵问题. (好像因为 $I$ 是单位矩阵, 只有对角线有值, 所以名字叫岭回归.)</p><p>另外为了从物理角度理解岭回归, 上述的损失函数还可以等价为:</p>\[f(w)=\sum_{i=1}^m\left(y_i-x_i^Tw\right)^2\\\notag s.t.\sum_{i=1}^nw_i^2\leq t\]<p>可以理解为在一个约束条件下的最小二乘法. 假设变量的个数为两个, 那么残差平方和就可以表示为 $w_1, w_2$ 的一个二次函数, 它是一个在三维空间中的抛物面, 可以使用等值线表示. 如果没有限制条件, 那么通过梯度下降法一定能够找到位于椭圆中心的那个最优值点. 而限制条件 $w_1^2+w_2^2\leq t$ 则相当于二维平面中的一个圆, 限制了两个变量的取值范围. 所以当等值线与圆相切的时候, 才能得到了约束条件下的最优点.</p><p><img src="https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20210620101826.png" alt="ridge_geometry" style="zoom: 50%;"></p><hr><p>在 Scikit-learn 中, 实现岭回归的方式是 <code class="language-plaintext highlighter-rouge">Ridge</code> 模型. 使用方法和 <code class="language-plaintext highlighter-rouge">LinearRegression</code> 基本一致, 只是需要一个岭参数 $\alpha$, 这里不再赘述. 需要关注的官方文档中给出的一个参数值随正则化参数 $\alpha$ 变化而变化的趋势图(<a href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html#sphx-glr-auto-examples-linear-model-plot-ridge-path-py" target="_blank" rel="noopener noreferrer">参考连接</a>):</p><p><img src="https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20210717203842.png" alt="sphx_glr_plot_ridge_path_001" style="zoom: 80%;"></p><p>这个图可以看出, 对于一个病态矩阵的问题 (任何一个元素的些许变动, 比如说数据的噪声, 导致整个矩阵的行列式值和逆矩阵发生很大的变化), 当正则化参数取值很大时(图的左侧), 损失函数受到惩罚项的影响就会占到主要作用, 各个特征参数都表现会<u>接近于 0</u>, 呈现出欠拟合状态; 而取值很小时(接近于0, 图的右侧), 损失函数则趋于最小二乘法的损失函数, 呈现出过拟合状态, 并且此时特征参数表现出了很大的震荡.</p><p>这再次说明了最小二乘法在面临病态矩阵问题时, 受到数据噪声的影响太大. 而岭回归则通过引入L2范数的惩罚项使得影响减弱. <strong>正是这个特性, 使得岭回归相比于最小二乘法更适用于共线性和病态数据的拟合, 常用于多维问题.</strong> 不过同时需要注意的是, 作为超参数的 $\alpha$, 取值大小的平衡是非常重要的. 而 Scikit-learn 中内置的 <code class="language-plaintext highlighter-rouge">RidgeCV</code> 是集成了交叉验证的岭回归模型训练器, 能够帮助我们找到适合的正则化参数.</p><h1 id="lasso">Lasso</h1><p>也叫套索回归, 也是OLS的改良版. 和岭回归的区别是, Lasso向损失函数增加一个L1范数的惩罚项.</p>\[\begin{align*} &amp;\min_w\left\|Xw-y\right\|_2^2+\lambda\left\|w\right\|_1^2\\ =&amp;\min_w\sum_{i=1}^m\left(y_i-x_i^Tw\right)^2+\lambda\sum_{i=1}^n|w_i| \end{align*}\]<p>因为L1范数是绝对值的形式, 损失函数在零点处不可导, 所以不能求得解析解. 所以使用梯度下降法求解最小值. 和岭回归一样, 从物理角度去看, 上述的式子等价为:</p>\[f(w)=\sum_{i=1}^m\left(y_i-x_i^Tw\right)^2\\\notag s.t.\sum_{i=1}^n|w_i|\leq t\]<p>也是一个在约束条件下求解最小值的问题. 假设变量也是两个, 则限制条件 $|w_1|+|w_2|\leq t$ 则相当于二维平面中的一个矩形. 两者相交时得到最优点.</p><p><img src="https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20210711235439.png" alt="Lasso_geometry" style="zoom:50%;"></p><p>Scikit-learn中使用 <code class="language-plaintext highlighter-rouge">Lasso</code> 模型实现 Lasso 算法. 使用方法也是传入一个正则化参数即可, 这里不赘述.</p><p>需要注意的是, 与岭回归不同的地方在于, Lasso回归引入的L1范数惩罚项会使得某些特征参数的值<u>变成 0</u>(岭回归只会使得参数更容易趋于0但不等于0). 那么为什么L1范数会比L2范数更能使得参数为 0 呢? 因为从上面的两张物理意义图中可以看出, 概率上方形的约束条件更容在坐标轴的顶点上与等值线产生交点. 圆形则是更容易在除坐标轴以外的地方产生相切点.</p><p>由于上述的特征, Lasso算法可以通过调整正则化参数, 使得一些特征量的权重参数为 0, 也就是删除这些特征量 (这也会使得计算量会变小). <strong>因此Lasso回归也能够用来做特征选择.</strong> Lasso名字中的 Selection Operator 也指出了这一点. 另外和岭回归一样, 通过交叉验证, 选择出一个合适的正则化参数也是非常重要的.</p><h1 id="elastic-net-202187追记">Elastic-Net (2021/8/7追记)</h1><p>弹性网络算法是上述岭回归和Lasso回归的混合体或者说是折衷方案, 同时使用了 L1 正则化和 L2 正则化, 也确实能够达到两个算法的效果. 以公式说明:</p>\[\begin{align*} &amp;\min_w\left\|Xw-y\right\|_2^2+\lambda_2\left\|w\right\|_2^2+\lambda_1\left\|w\right\|_1\\ =&amp;\min_w\sum_{i=1}^m\left(y_i-x_i^Tw\right)^2+\lambda_2\sum_{i=1}^nw_i^2+\lambda_1\sum_{i=1}^n|w_i| \end{align*}\]<p>弹性网络算法既能向 Lasso 回归一样处理疏矩阵的问题, 也能够保留岭回归处理病态数据和共线性问题的优点, 这里不展开具体的讨论.</p><p>Scikit-Learn中提供了 <code class="language-plaintext highlighter-rouge">ElasticNet</code> 用于实现弹性网络回归模型.</p><hr><p>以上就是线性模型中常用的基本的四个算法. 写的内容不是很难, 但是在写的过程中我发现文章当中”统计模型” 以及 “机器学习” 的内容似乎混杂在了一起. 这不禁让我对这两者的区别产生了思考, 所以这篇文章花了相当长的时间. 估计之后会写一篇新的文章, 以总结自己针对两者的区别的调查和思考.</p><div class="footnotes" role="doc-endnotes"><ol><li id="fn:1" role="doc-endnote"><p>这篇文章暂时不涉及求取最小值的方法, 多是采用梯度下降法. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p></li></ol></div></div><hr><div class="post-nav"><a href="/2021/05/09/summary-of-machine-learning-algorithms-logistic-regression" class="left link">&lt;&lt; Previous </a><a href="/2021/07/23/machine-learning-vs-statistics" class="right link">Next &gt;&gt;</a></div><hr><div id="disqus_thread"></div><script>/**
  *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
  *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
  */
  /*
  var disqus_config = function () {
  this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
  this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  */
  (function() {  // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');

  s.src = '//simcookiesgithubio.disqus.com/embed.js';

  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();</script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="noopener noreferrer" target="_blank">comments powered by Disqus.</a></noscript></article><div class="mark-link"><a id="bookmark"><span class="fa-stack fa-lg"><i class="fa fa-square fa-stack-2x"></i> <i class="fa fa-book fa-stack-1x fa-inverse"></i></span></a></div><div id="toc-area" class="sidebar" style="display:none"><div class="col toc-area">  <i class="fa fa-bookmark" aria-hidden="true"></i>  Table of Contents<ul class="section-nav"><li class="toc-entry toc-h1"><a href="#%E6%99%AE%E9%80%9A%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95">普通最小二乘法</a></li><li class="toc-entry toc-h1"><a href="#%E5%B2%AD%E5%9B%9E%E5%BD%92">岭回归</a></li><li class="toc-entry toc-h1"><a href="#lasso">Lasso</a></li><li class="toc-entry toc-h1"><a href="#elastic-net-202187%E8%BF%BD%E8%AE%B0">Elastic-Net (2021/8/7追记)</a></li></ul></div></div></div></div><footer class="site-footer">© 2016~2021 by Simcookies. All rights reserved.<div class="quick-link" id="quick-link"><a id="page-top"><span class="fa-stack fa-lg"><i class="fa fa-square fa-stack-2x"></i> <i class="fa fa-arrow-up fa-stack-1x fa-inverse"></i> </span></a><a href="/"><span class="fa-stack fa-lg"><i class="fa fa-square fa-stack-2x"></i> <i class="fa fa-home fa-stack-1x fa-inverse"></i></span></a></div></footer></div><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                           m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-85209586-1', 'auto');
  ga('send', 'pageview');</script></body></html>